[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Linear Algebra",
    "section": "",
    "text": "Preface\nWelcome to Advanced Linear Algebra Notes, a comprehensive guide to spectral theory and optimization techniques in linear algebra.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Linear Algebra",
    "section": "About This Book",
    "text": "About This Book\nThis book provides a rigorous yet accessible introduction to advanced topics in linear algebra, with a focus on:\n\nSpectral Theory: Eigenvalues, eigenvectors, and matrix decompositions\nOptimization: Convex optimization and its applications\nComputational Methods: Algorithms for numerical linear algebra",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Linear Algebra",
    "section": "How to Use This Book",
    "text": "How to Use This Book\nEach chapter contains:\n\nDefinitions with precise mathematical statements\nTheorems with complete proofs\nExamples illustrating key concepts\nExercises for practice",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Linear Algebra",
    "section": "Prerequisites",
    "text": "Prerequisites\nReaders should be familiar with:\n\nBasic linear algebra (vectors, matrices, linear transformations)\nCalculus (derivatives, integrals)\nBasic proof techniques",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Linear Algebra",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis book was created using Quarto with a custom LaTeX template for beautiful mathematical typesetting.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "src/ch00-foundations/index.html",
    "href": "src/ch00-foundations/index.html",
    "title": "Preliminary",
    "section": "",
    "text": "This chapter covers the essential background needed for the study of linear algebra. Feel free to skip sections that you are already familiar with. In addition to the following knowledge, basic knowledge of logic and proof techniques is recommended, including:\n\nDirect proof\nProof by contradiction\nProof by induction\nProof by contrapositive\nProof by cases (exhaustion)\nConstructive proof (constructive existence)\nNon-constructive existence proof\nUniqueness proof\nProof of equivalence (iff proof)\nDisproof by counterexample\nPigeonhole principle",
    "crumbs": [
      "Preliminary"
    ]
  },
  {
    "objectID": "src/ch00-foundations/01-sets-and-notation.html",
    "href": "src/ch00-foundations/01-sets-and-notation.html",
    "title": "1  Sets and Notation",
    "section": "",
    "text": "$$\n\\def\\nR{\\mathbb{R}}\n\\def\\nC{\\mathbb{C}}\n\\def\\nN{\\mathbb{N}}\n\\def\\nZ{\\mathbb{Z}}\n\\def\\nQ{\\mathbb{Q}}\n\\def\\nF{\\mathbb{F}}\n\\def\\nP{\\mathbb{P}}\n\\def\\nE{\\mathbb{E}}\n\\def\\sA{\\mathcal{A}}\n\\def\\sB{\\mathcal{B}}\n\\def\\sC{\\mathcal{C}}\n\\def\\sD{\\mathcal{D}}\n\\def\\sE{\\mathcal{E}}\n\\def\\sF{\\mathcal{F}}\n\\def\\sG{\\mathcal{G}}\n\\def\\sH{\\mathcal{H}}\n\\def\\sL{\\mathcal{L}}\n\\def\\sM{\\mathcal{M}}\n\\def\\sN{\\mathcal{N}}\n\\def\\sO{\\mathcal{O}}\n\\def\\sP{\\mathcal{P}}\n\\def\\sS{\\mathcal{S}}\n\\def\\sT{\\mathcal{T}}\n\\def\\sX{\\mathcal{X}}\n\\def\\sY{\\mathcal{Y}}\n\\def\\va{\\mathbf{a}}\n\\def\\vb{\\mathbf{b}}\n\\def\\vc{\\mathbf{c}}\n\\def\\vd{\\mathbf{d}}\n\\def\\ve{\\mathbf{e}}\n\\def\\vf{\\mathbf{f}}\n\\def\\vg{\\mathbf{g}}\n\\def\\vh{\\mathbf{h}}\n\\def\\vi{\\mathbf{i}}\n\\def\\vj{\\mathbf{j}}\n\\def\\vk{\\mathbf{k}}\n\\def\\vl{\\mathbf{l}}\n\\def\\vm{\\mathbf{m}}\n\\def\\vn{\\mathbf{n}}\n\\def\\vo{\\mathbf{o}}\n\\def\\vp{\\mathbf{p}}\n\\def\\vq{\\mathbf{q}}\n\\def\\vr{\\mathbf{r}}\n\\def\\vs{\\mathbf{s}}\n\\def\\vt{\\mathbf{t}}\n\\def\\vu{\\mathbf{u}}\n\\def\\vv{\\mathbf{v}}\n\\def\\vw{\\mathbf{w}}\n\\def\\vx{\\mathbf{x}}\n\\def\\vy{\\mathbf{y}}\n\\def\\vz{\\mathbf{z}}\n\\def\\mA{\\mathbf{A}}\n\\def\\mB{\\mathbf{B}}\n\\def\\mC{\\mathbf{C}}\n\\def\\mD{\\mathbf{D}}\n\\def\\mE{\\mathbf{E}}\n\\def\\mF{\\mathbf{F}}\n\\def\\mG{\\mathbf{G}}\n\\def\\mH{\\mathbf{H}}\n\\def\\mI{\\mathbf{I}}\n\\def\\mJ{\\mathbf{J}}\n\\def\\mK{\\mathbf{K}}\n\\def\\mL{\\mathbf{L}}\n\\def\\mM{\\mathbf{M}}\n\\def\\mN{\\mathbf{N}}\n\\def\\mO{\\mathbf{O}}\n\\def\\mP{\\mathbf{P}}\n\\def\\mQ{\\mathbf{Q}}\n\\def\\mR{\\mathbf{R}}\n\\def\\mS{\\mathbf{S}}\n\\def\\mT{\\mathbf{T}}\n\\def\\mU{\\mathbf{U}}\n\\def\\mV{\\mathbf{V}}\n\\def\\mW{\\mathbf{W}}\n\\def\\mX{\\mathbf{X}}\n\\def\\mY{\\mathbf{Y}}\n\\def\\mZ{\\mathbf{Z}}\n\\def\\A{\\mathbf{A}}\n\\def\\B{\\mathbf{B}}\n\\def\\C{\\mathbf{C}}\n\\def\\D{\\mathbf{D}}\n\\def\\E{\\mathbf{E}}\n\\def\\F{\\mathbf{F}}\n\\def\\G{\\mathbf{G}}\n\\def\\H{\\mathbf{H}}\n\\def\\I{\\mathbf{I}}\n\\def\\J{\\mathbf{J}}\n\\def\\K{\\mathbf{K}}\n\\def\\L{\\mathbf{L}}\n\\def\\M{\\mathbf{M}}\n\\def\\N{\\mathbf{N}}\n\\def\\O{\\mathbf{O}}\n\\def\\P{\\mathbf{P}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\R{\\mathbf{R}}\n\\def\\S{\\mathbf{S}}\n\\def\\T{^\\top}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\W{\\mathbf{W}}\n\\def\\X{\\mathbf{X}}\n\\def\\Y{\\mathbf{Y}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\valpha{\\boldsymbol{\\alpha}}\n\\def\\vbeta{\\boldsymbol{\\beta}}\n\\def\\vgamma{\\boldsymbol{\\gamma}}\n\\def\\vdelta{\\boldsymbol{\\delta}}\n\\def\\vepsilon{\\boldsymbol{\\epsilon}}\n\\def\\vzeta{\\boldsymbol{\\zeta}}\n\\def\\veta{\\boldsymbol{\\eta}}\n\\def\\vtheta{\\boldsymbol{\\theta}}\n\\def\\viota{\\boldsymbol{\\iota}}\n\\def\\vkappa{\\boldsymbol{\\kappa}}\n\\def\\vlambda{\\boldsymbol{\\lambda}}\n\\def\\vmu{\\boldsymbol{\\mu}}\n\\def\\vnu{\\boldsymbol{\\nu}}\n\\def\\vxi{\\boldsymbol{\\xi}}\n\\def\\vpi{\\boldsymbol{\\pi}}\n\\def\\vrho{\\boldsymbol{\\rho}}\n\\def\\vsigma{\\boldsymbol{\\sigma}}\n\\def\\vtau{\\boldsymbol{\\tau}}\n\\def\\vupsilon{\\boldsymbol{\\upsilon}}\n\\def\\vphi{\\boldsymbol{\\phi}}\n\\def\\vchi{\\boldsymbol{\\chi}}\n\\def\\vpsi{\\boldsymbol{\\psi}}\n\\def\\vomega{\\boldsymbol{\\omega}}\n\\def\\vGamma{\\boldsymbol{\\Gamma}}\n\\def\\vDelta{\\boldsymbol{\\Delta}}\n\\def\\vTheta{\\boldsymbol{\\Theta}}\n\\def\\vLambda{\\boldsymbol{\\Lambda}}\n\\def\\vXi{\\boldsymbol{\\Xi}}\n\\def\\vPi{\\boldsymbol{\\Pi}}\n\\def\\vSigma{\\boldsymbol{\\Sigma}}\n\\def\\vUpsilon{\\boldsymbol{\\Upsilon}}\n\\def\\vPhi{\\boldsymbol{\\Phi}}\n\\def\\vPsi{\\boldsymbol{\\Psi}}\n\\def\\vOmega{\\boldsymbol{\\Omega}}\n\\def\\ra{\\mathbf{a}}\n\\def\\rb{\\mathbf{b}}\n\\def\\rc{\\mathbf{c}}\n\\def\\rd{\\mathbf{d}}\n\\def\\re{\\mathbf{e}}\n\\def\\rf{\\mathbf{f}}\n\\def\\rg{\\mathbf{g}}\n\\def\\rh{\\mathbf{h}}\n\\def\\ri{\\mathbf{i}}\n\\def\\rj{\\mathbf{j}}\n\\def\\rk{\\mathbf{k}}\n\\def\\rl{\\mathbf{l}}\n\\def\\rm{\\mathbf{m}}\n\\def\\rn{\\mathbf{n}}\n\\def\\ro{\\mathbf{o}}\n\\def\\rp{\\mathbf{p}}\n\\def\\rq{\\mathbf{q}}\n\\def\\rr{\\mathbf{r}}\n\\def\\rs{\\mathbf{s}}\n\\def\\rt{\\mathbf{t}}\n\\def\\ru{\\mathbf{u}}\n\\def\\rv{\\mathbf{v}}\n\\def\\rw{\\mathbf{w}}\n\\def\\rx{\\mathbf{x}}\n\\def\\ry{\\mathbf{y}}\n\\def\\rz{\\mathbf{z}}\n\\def\\vzero{\\mathbf{0}}\n\\def\\vone{\\mathbf{1}}\n\\def\\Ev{\\mathbb{E}}\n\\def\\Pr{\\operatorname{Pr}}\n\\def\\Var{\\operatorname{Var}}\n\\def\\Cov{\\operatorname{Cov}}\n\\def\\Cor{\\operatorname{Cor}}\n\\def\\Corr{\\operatorname{Corr}}\n\\def\\distBern{\\operatorname{Ber}}\n\\def\\distBin{\\operatorname{Bin}}\n\\def\\distGeom{\\operatorname{Geo}}\n\\def\\distNB{\\operatorname{NB}}\n\\def\\distPois{\\operatorname{Poi}}\n\\def\\distHyp{\\operatorname{Hyp}}\n\\def\\distUnif{\\operatorname{Unif}}\n\\def\\distExp{\\operatorname{Exp}}\n\\def\\distGamma{\\operatorname{Gamma}}\n\\def\\distN{\\mathcal{N}}\n\\def\\distBeta{\\operatorname{Beta}}\n\\def\\distCauchy{\\operatorname{Cauchy}}\n\\def\\distDir{\\operatorname{Dir}}\n\\def\\distMult{\\operatorname{Mult}}\n\\def\\distWishart{\\operatorname{Wishart}}\n\\def\\tr{\\operatorname{tr}}\n\\def\\trace{\\operatorname{tr}}\n\\def\\rank{\\operatorname{rank}}\n\\def\\nullity{\\operatorname{nullity}}\n\\def\\Span{\\operatorname{span}}\n\\def\\spn{\\operatorname{span}}\n\\def\\col{\\operatorname{col}}\n\\def\\row{\\operatorname{row}}\n\\def\\nul{\\operatorname{nul}}\n\\def\\img{\\operatorname{img}}\n\\def\\diag{\\operatorname{diag}}\n\\def\\dd{\\mathrm{d}}\n\\def\\transpose{^\\top}\n\\def\\inv{^{-1}}\n\\def\\abs#1{\\left\\lvert#1\\right\\rvert}\n\\def\\norm#1{\\left\\lVert#1\\right\\rVert}\n\\def\\floor#1{\\left\\lfloor#1\\right\\rfloor}\n\\def\\ceil#1{\\left\\lceil#1\\right\\rceil}\n\\def\\inner#1#2{\\langle #1, #2 \\rangle}\n\\def\\lcm{\\operatorname{lcm}}\n\\def\\im{\\operatorname{im}}\n\\def\\sgn{\\operatorname{sgn}}\n\\def\\argmax{\\operatorname{argmax}}\n\\def\\argmin{\\operatorname{argmin}}\n\\def\\softmax{\\operatorname{softmax}}\n\\def\\x{\\vx}\n\\def\\y{\\vy}\n\\def\\z{\\vz}\n\\def\\w{\\vw}\n\\def\\u{\\vu}\n\\def\\e{\\ve}\n\\def\\ba{\\mathbf{a}}\n\\def\\bb{\\mathbf{b}}\n\\def\\bc{\\mathbf{c}}\n\\def\\bd{\\mathbf{d}}\n\\def\\be{\\mathbf{e}}\n\\def\\bff{\\mathbf{f}}\n\\def\\bg{\\mathbf{g}}\n\\def\\bh{\\mathbf{h}}\n\\def\\bi{\\mathbf{i}}\n\\def\\bj{\\mathbf{j}}\n\\def\\bk{\\mathbf{k}}\n\\def\\bl{\\mathbf{l}}\n\\def\\bm{\\mathbf{m}}\n\\def\\bn{\\mathbf{n}}\n\\def\\bo{\\mathbf{o}}\n\\def\\bp{\\mathbf{p}}\n\\def\\bq{\\mathbf{q}}\n\\def\\br{\\mathbf{r}}\n\\def\\bs{\\mathbf{s}}\n\\def\\bt{\\mathbf{t}}\n\\def\\bu{\\mathbf{u}}\n\\def\\bv{\\mathbf{v}}\n\\def\\bw{\\mathbf{w}}\n\\def\\bx{\\mathbf{x}}\n\\def\\by{\\mathbf{y}}\n\\def\\bz{\\mathbf{z}}\n\\def\\bA{\\mathbf{A}}\n\\def\\bB{\\mathbf{B}}\n\\def\\bC{\\mathbf{C}}\n\\def\\bD{\\mathbf{D}}\n\\def\\bE{\\mathbf{E}}\n\\def\\bF{\\mathbf{F}}\n\\def\\bG{\\mathbf{G}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bJ{\\mathbf{J}}\n\\def\\bK{\\mathbf{K}}\n\\def\\bL{\\mathbf{L}}\n\\def\\bM{\\mathbf{M}}\n\\def\\bN{\\mathbf{N}}\n\\def\\bO{\\mathbf{O}}\n\\def\\bP{\\mathbf{P}}\n\\def\\bQ{\\mathbf{Q}}\n\\def\\bR{\\mathbf{R}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bT{\\mathbf{T}}\n\\def\\bU{\\mathbf{U}}\n\\def\\bV{\\mathbf{V}}\n\\def\\bW{\\mathbf{W}}\n\\def\\bX{\\mathbf{X}}\n\\def\\bY{\\mathbf{Y}}\n\\def\\bZ{\\mathbf{Z}}\n\\def\\bSigma{\\mathbf{\\Sigma}}\n\\def\\bLambda{\\mathbf{\\Lambda}}\n\\def\\bOmega{\\mathbf{\\Omega}}\n$$\n\nWe begin by defining the standard sets of numbers used throughout this book.\n\nDefinition 1.1 (Common Number Sets) We use the following notation for common sets of numbers:\n\n\\(\\nN\\): The set of natural numbers \\(\\{1, 2, 3, \\ldots\\}\\).\n\\(\\nZ\\): The set of integers \\(\\{\\ldots, -2, -1, 0, 1, 2, \\ldots\\}\\).\n\\(\\nQ\\): The set of rational numbers.\n\\(\\nR\\): The set of real numbers.\n\\(\\nC\\): The set of complex numbers.\n\n\n\nDefinition 1.2 (Cartesian Product) Let \\(A\\) and \\(B\\) be sets. The Cartesian product of \\(A\\) and \\(B\\), denoted \\(A \\times B\\), is the set of all ordered pairs: \\[\n    A \\times B = \\{(a, b) : a \\in A, b \\in B\\}\n\\] More generally, for sets \\(A_1, A_2, \\ldots, A_n\\), the Cartesian product is: \\[\n    A_1 \\times A_2 \\times \\cdots \\times A_n = \\{(a_1, a_2, \\ldots, a_n) : a_i \\in A_i \\text{ for } i = 1, 2, \\ldots, n\\}\n\\]\n\n\nDefinition 1.3 (\\(n\\)-Tuple) An \\(n\\)-tuple is an ordered list of \\(n\\) elements. For a set \\(A\\), the set of all \\(n\\)-tuples with entries from \\(A\\) is denoted: \\[\n    A^n = \\underbrace{A \\times A \\times \\cdots \\times A}_{n \\text{ times}} = \\{(a_1, a_2, \\ldots, a_n) : a_i \\in A \\text{ for } i = 1, 2, \\ldots, n\\}\n\\]\n\n\nExample 1.1 (\\(n\\)-Tuples) Here are examples of \\(n\\)-tuples from various sets:\n\n\\(\\nR^2\\): The Cartesian plane. Elements include \\((0, 0)\\), \\((1, 2)\\), \\((-3, \\pi)\\), \\((\\sqrt{2}, -5.7)\\).\n\\(\\nR^3\\): 3-dimensional space. Elements include \\((1, 0, 0)\\), \\((1, 2, 3)\\), \\((-1, \\pi, e)\\).\n\\(\\nZ^2\\): Pairs of integers. Elements include \\((0, 0)\\), \\((1, -2)\\), \\((5, 7)\\). Note that \\((\\frac{1}{2}, 3) \\notin \\nZ^2\\).\n\\(\\nC^2\\): Pairs of complex numbers. Elements include \\((1+i, 2-3i)\\), \\((i, 0)\\), \\((3, 4)\\).\n\\(\\{0, 1\\}^3\\): Binary 3-tuples. This set has exactly 8 elements: \\[\n\\{0,1\\}^3 = \\{(0,0,0), (0,0,1), (0,1,0), (0,1,1), (1,0,0), (1,0,1), (1,1,0), (1,1,1)\\}\n\\]\n\nIn general, if \\(A\\) is a finite set with \\(|A| = k\\) elements, then \\(A^n\\) has \\(k^n\\) elements.",
    "crumbs": [
      "Preliminary",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sets and Notation</span>"
    ]
  },
  {
    "objectID": "src/ch00-foundations/02-functions.html",
    "href": "src/ch00-foundations/02-functions.html",
    "title": "2  Functions",
    "section": "",
    "text": "$$\n\\def\\nR{\\mathbb{R}}\n\\def\\nC{\\mathbb{C}}\n\\def\\nN{\\mathbb{N}}\n\\def\\nZ{\\mathbb{Z}}\n\\def\\nQ{\\mathbb{Q}}\n\\def\\nF{\\mathbb{F}}\n\\def\\nP{\\mathbb{P}}\n\\def\\nE{\\mathbb{E}}\n\\def\\sA{\\mathcal{A}}\n\\def\\sB{\\mathcal{B}}\n\\def\\sC{\\mathcal{C}}\n\\def\\sD{\\mathcal{D}}\n\\def\\sE{\\mathcal{E}}\n\\def\\sF{\\mathcal{F}}\n\\def\\sG{\\mathcal{G}}\n\\def\\sH{\\mathcal{H}}\n\\def\\sL{\\mathcal{L}}\n\\def\\sM{\\mathcal{M}}\n\\def\\sN{\\mathcal{N}}\n\\def\\sO{\\mathcal{O}}\n\\def\\sP{\\mathcal{P}}\n\\def\\sS{\\mathcal{S}}\n\\def\\sT{\\mathcal{T}}\n\\def\\sX{\\mathcal{X}}\n\\def\\sY{\\mathcal{Y}}\n\\def\\va{\\mathbf{a}}\n\\def\\vb{\\mathbf{b}}\n\\def\\vc{\\mathbf{c}}\n\\def\\vd{\\mathbf{d}}\n\\def\\ve{\\mathbf{e}}\n\\def\\vf{\\mathbf{f}}\n\\def\\vg{\\mathbf{g}}\n\\def\\vh{\\mathbf{h}}\n\\def\\vi{\\mathbf{i}}\n\\def\\vj{\\mathbf{j}}\n\\def\\vk{\\mathbf{k}}\n\\def\\vl{\\mathbf{l}}\n\\def\\vm{\\mathbf{m}}\n\\def\\vn{\\mathbf{n}}\n\\def\\vo{\\mathbf{o}}\n\\def\\vp{\\mathbf{p}}\n\\def\\vq{\\mathbf{q}}\n\\def\\vr{\\mathbf{r}}\n\\def\\vs{\\mathbf{s}}\n\\def\\vt{\\mathbf{t}}\n\\def\\vu{\\mathbf{u}}\n\\def\\vv{\\mathbf{v}}\n\\def\\vw{\\mathbf{w}}\n\\def\\vx{\\mathbf{x}}\n\\def\\vy{\\mathbf{y}}\n\\def\\vz{\\mathbf{z}}\n\\def\\mA{\\mathbf{A}}\n\\def\\mB{\\mathbf{B}}\n\\def\\mC{\\mathbf{C}}\n\\def\\mD{\\mathbf{D}}\n\\def\\mE{\\mathbf{E}}\n\\def\\mF{\\mathbf{F}}\n\\def\\mG{\\mathbf{G}}\n\\def\\mH{\\mathbf{H}}\n\\def\\mI{\\mathbf{I}}\n\\def\\mJ{\\mathbf{J}}\n\\def\\mK{\\mathbf{K}}\n\\def\\mL{\\mathbf{L}}\n\\def\\mM{\\mathbf{M}}\n\\def\\mN{\\mathbf{N}}\n\\def\\mO{\\mathbf{O}}\n\\def\\mP{\\mathbf{P}}\n\\def\\mQ{\\mathbf{Q}}\n\\def\\mR{\\mathbf{R}}\n\\def\\mS{\\mathbf{S}}\n\\def\\mT{\\mathbf{T}}\n\\def\\mU{\\mathbf{U}}\n\\def\\mV{\\mathbf{V}}\n\\def\\mW{\\mathbf{W}}\n\\def\\mX{\\mathbf{X}}\n\\def\\mY{\\mathbf{Y}}\n\\def\\mZ{\\mathbf{Z}}\n\\def\\A{\\mathbf{A}}\n\\def\\B{\\mathbf{B}}\n\\def\\C{\\mathbf{C}}\n\\def\\D{\\mathbf{D}}\n\\def\\E{\\mathbf{E}}\n\\def\\F{\\mathbf{F}}\n\\def\\G{\\mathbf{G}}\n\\def\\H{\\mathbf{H}}\n\\def\\I{\\mathbf{I}}\n\\def\\J{\\mathbf{J}}\n\\def\\K{\\mathbf{K}}\n\\def\\L{\\mathbf{L}}\n\\def\\M{\\mathbf{M}}\n\\def\\N{\\mathbf{N}}\n\\def\\O{\\mathbf{O}}\n\\def\\P{\\mathbf{P}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\R{\\mathbf{R}}\n\\def\\S{\\mathbf{S}}\n\\def\\T{^\\top}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\W{\\mathbf{W}}\n\\def\\X{\\mathbf{X}}\n\\def\\Y{\\mathbf{Y}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\valpha{\\boldsymbol{\\alpha}}\n\\def\\vbeta{\\boldsymbol{\\beta}}\n\\def\\vgamma{\\boldsymbol{\\gamma}}\n\\def\\vdelta{\\boldsymbol{\\delta}}\n\\def\\vepsilon{\\boldsymbol{\\epsilon}}\n\\def\\vzeta{\\boldsymbol{\\zeta}}\n\\def\\veta{\\boldsymbol{\\eta}}\n\\def\\vtheta{\\boldsymbol{\\theta}}\n\\def\\viota{\\boldsymbol{\\iota}}\n\\def\\vkappa{\\boldsymbol{\\kappa}}\n\\def\\vlambda{\\boldsymbol{\\lambda}}\n\\def\\vmu{\\boldsymbol{\\mu}}\n\\def\\vnu{\\boldsymbol{\\nu}}\n\\def\\vxi{\\boldsymbol{\\xi}}\n\\def\\vpi{\\boldsymbol{\\pi}}\n\\def\\vrho{\\boldsymbol{\\rho}}\n\\def\\vsigma{\\boldsymbol{\\sigma}}\n\\def\\vtau{\\boldsymbol{\\tau}}\n\\def\\vupsilon{\\boldsymbol{\\upsilon}}\n\\def\\vphi{\\boldsymbol{\\phi}}\n\\def\\vchi{\\boldsymbol{\\chi}}\n\\def\\vpsi{\\boldsymbol{\\psi}}\n\\def\\vomega{\\boldsymbol{\\omega}}\n\\def\\vGamma{\\boldsymbol{\\Gamma}}\n\\def\\vDelta{\\boldsymbol{\\Delta}}\n\\def\\vTheta{\\boldsymbol{\\Theta}}\n\\def\\vLambda{\\boldsymbol{\\Lambda}}\n\\def\\vXi{\\boldsymbol{\\Xi}}\n\\def\\vPi{\\boldsymbol{\\Pi}}\n\\def\\vSigma{\\boldsymbol{\\Sigma}}\n\\def\\vUpsilon{\\boldsymbol{\\Upsilon}}\n\\def\\vPhi{\\boldsymbol{\\Phi}}\n\\def\\vPsi{\\boldsymbol{\\Psi}}\n\\def\\vOmega{\\boldsymbol{\\Omega}}\n\\def\\ra{\\mathbf{a}}\n\\def\\rb{\\mathbf{b}}\n\\def\\rc{\\mathbf{c}}\n\\def\\rd{\\mathbf{d}}\n\\def\\re{\\mathbf{e}}\n\\def\\rf{\\mathbf{f}}\n\\def\\rg{\\mathbf{g}}\n\\def\\rh{\\mathbf{h}}\n\\def\\ri{\\mathbf{i}}\n\\def\\rj{\\mathbf{j}}\n\\def\\rk{\\mathbf{k}}\n\\def\\rl{\\mathbf{l}}\n\\def\\rm{\\mathbf{m}}\n\\def\\rn{\\mathbf{n}}\n\\def\\ro{\\mathbf{o}}\n\\def\\rp{\\mathbf{p}}\n\\def\\rq{\\mathbf{q}}\n\\def\\rr{\\mathbf{r}}\n\\def\\rs{\\mathbf{s}}\n\\def\\rt{\\mathbf{t}}\n\\def\\ru{\\mathbf{u}}\n\\def\\rv{\\mathbf{v}}\n\\def\\rw{\\mathbf{w}}\n\\def\\rx{\\mathbf{x}}\n\\def\\ry{\\mathbf{y}}\n\\def\\rz{\\mathbf{z}}\n\\def\\vzero{\\mathbf{0}}\n\\def\\vone{\\mathbf{1}}\n\\def\\Ev{\\mathbb{E}}\n\\def\\Pr{\\operatorname{Pr}}\n\\def\\Var{\\operatorname{Var}}\n\\def\\Cov{\\operatorname{Cov}}\n\\def\\Cor{\\operatorname{Cor}}\n\\def\\Corr{\\operatorname{Corr}}\n\\def\\distBern{\\operatorname{Ber}}\n\\def\\distBin{\\operatorname{Bin}}\n\\def\\distGeom{\\operatorname{Geo}}\n\\def\\distNB{\\operatorname{NB}}\n\\def\\distPois{\\operatorname{Poi}}\n\\def\\distHyp{\\operatorname{Hyp}}\n\\def\\distUnif{\\operatorname{Unif}}\n\\def\\distExp{\\operatorname{Exp}}\n\\def\\distGamma{\\operatorname{Gamma}}\n\\def\\distN{\\mathcal{N}}\n\\def\\distBeta{\\operatorname{Beta}}\n\\def\\distCauchy{\\operatorname{Cauchy}}\n\\def\\distDir{\\operatorname{Dir}}\n\\def\\distMult{\\operatorname{Mult}}\n\\def\\distWishart{\\operatorname{Wishart}}\n\\def\\tr{\\operatorname{tr}}\n\\def\\trace{\\operatorname{tr}}\n\\def\\rank{\\operatorname{rank}}\n\\def\\nullity{\\operatorname{nullity}}\n\\def\\Span{\\operatorname{span}}\n\\def\\spn{\\operatorname{span}}\n\\def\\col{\\operatorname{col}}\n\\def\\row{\\operatorname{row}}\n\\def\\nul{\\operatorname{nul}}\n\\def\\img{\\operatorname{img}}\n\\def\\diag{\\operatorname{diag}}\n\\def\\dd{\\mathrm{d}}\n\\def\\transpose{^\\top}\n\\def\\inv{^{-1}}\n\\def\\abs#1{\\left\\lvert#1\\right\\rvert}\n\\def\\norm#1{\\left\\lVert#1\\right\\rVert}\n\\def\\floor#1{\\left\\lfloor#1\\right\\rfloor}\n\\def\\ceil#1{\\left\\lceil#1\\right\\rceil}\n\\def\\inner#1#2{\\langle #1, #2 \\rangle}\n\\def\\lcm{\\operatorname{lcm}}\n\\def\\im{\\operatorname{im}}\n\\def\\sgn{\\operatorname{sgn}}\n\\def\\argmax{\\operatorname{argmax}}\n\\def\\argmin{\\operatorname{argmin}}\n\\def\\softmax{\\operatorname{softmax}}\n\\def\\x{\\vx}\n\\def\\y{\\vy}\n\\def\\z{\\vz}\n\\def\\w{\\vw}\n\\def\\u{\\vu}\n\\def\\e{\\ve}\n\\def\\ba{\\mathbf{a}}\n\\def\\bb{\\mathbf{b}}\n\\def\\bc{\\mathbf{c}}\n\\def\\bd{\\mathbf{d}}\n\\def\\be{\\mathbf{e}}\n\\def\\bff{\\mathbf{f}}\n\\def\\bg{\\mathbf{g}}\n\\def\\bh{\\mathbf{h}}\n\\def\\bi{\\mathbf{i}}\n\\def\\bj{\\mathbf{j}}\n\\def\\bk{\\mathbf{k}}\n\\def\\bl{\\mathbf{l}}\n\\def\\bm{\\mathbf{m}}\n\\def\\bn{\\mathbf{n}}\n\\def\\bo{\\mathbf{o}}\n\\def\\bp{\\mathbf{p}}\n\\def\\bq{\\mathbf{q}}\n\\def\\br{\\mathbf{r}}\n\\def\\bs{\\mathbf{s}}\n\\def\\bt{\\mathbf{t}}\n\\def\\bu{\\mathbf{u}}\n\\def\\bv{\\mathbf{v}}\n\\def\\bw{\\mathbf{w}}\n\\def\\bx{\\mathbf{x}}\n\\def\\by{\\mathbf{y}}\n\\def\\bz{\\mathbf{z}}\n\\def\\bA{\\mathbf{A}}\n\\def\\bB{\\mathbf{B}}\n\\def\\bC{\\mathbf{C}}\n\\def\\bD{\\mathbf{D}}\n\\def\\bE{\\mathbf{E}}\n\\def\\bF{\\mathbf{F}}\n\\def\\bG{\\mathbf{G}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bJ{\\mathbf{J}}\n\\def\\bK{\\mathbf{K}}\n\\def\\bL{\\mathbf{L}}\n\\def\\bM{\\mathbf{M}}\n\\def\\bN{\\mathbf{N}}\n\\def\\bO{\\mathbf{O}}\n\\def\\bP{\\mathbf{P}}\n\\def\\bQ{\\mathbf{Q}}\n\\def\\bR{\\mathbf{R}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bT{\\mathbf{T}}\n\\def\\bU{\\mathbf{U}}\n\\def\\bV{\\mathbf{V}}\n\\def\\bW{\\mathbf{W}}\n\\def\\bX{\\mathbf{X}}\n\\def\\bY{\\mathbf{Y}}\n\\def\\bZ{\\mathbf{Z}}\n\\def\\bSigma{\\mathbf{\\Sigma}}\n\\def\\bLambda{\\mathbf{\\Lambda}}\n\\def\\bOmega{\\mathbf{\\Omega}}\n$$\n\n\nDefinition 2.1 (Function) A function \\(f\\) from a set \\(A\\) to a set \\(B\\), denoted \\(f: A \\to B\\), is a rule that assigns to each element \\(a \\in A\\) exactly one element \\(f(a) \\in B\\). The set \\(A\\) is called the domain and \\(B\\) is called the codomain.\n\n\nDefinition 2.2 (Injective, Surjective, Bijective) Let \\(f: A \\to B\\) be a function.\n\n\\(f\\) is injective (one-to-one) if \\(f(a_1) = f(a_2)\\) implies \\(a_1 = a_2\\).\n\\(f\\) is surjective (onto) if for every \\(b \\in B\\), there exists \\(a \\in A\\) such that \\(f(a) = b\\).\n\\(f\\) is bijective if it is both injective and surjective\n\n\n\nExample 2.1 (Injective, Surjective, and Bijective Functions) Consider the following functions:\n\n\\(f: \\nR \\to \\nR\\), \\(f(x) = 2x + 1\\) is bijective.\n\nInjective: If \\(2x_1 + 1 = 2x_2 + 1\\), then \\(x_1 = x_2\\).\nSurjective: For any \\(y \\in \\nR\\), we have \\(f\\left(\\frac{y-1}{2}\\right) = y\\).\n\n\\(g: \\nR \\to \\nR\\), \\(g(x) = x^2\\) is neither injective nor surjective.\n\nNot injective: \\(g(1) = g(-1) = 1\\), but \\(1 \\neq -1\\).\nNot surjective: There is no \\(x \\in \\nR\\) such that \\(g(x) = -1\\).\n\n\\(h: \\nR \\to [0, \\infty)\\), \\(h(x) = x^2\\) is surjective but not injective.\n\nNot injective: \\(h(1) = h(-1) = 1\\).\nSurjective: For any \\(y \\geq 0\\), we have \\(h(\\sqrt{y}) = y\\).\n\n\\(k: [0, \\infty) \\to \\nR\\), \\(k(x) = x^2\\) is injective but not surjective.\n\nInjective: If \\(x_1^2 = x_2^2\\) with \\(x_1, x_2 \\geq 0\\), then \\(x_1 = x_2\\).\nNot surjective: There is no \\(x \\geq 0\\) such that \\(k(x) = -1\\).",
    "crumbs": [
      "Preliminary",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "src/ch00-foundations/03-polynomials.html",
    "href": "src/ch00-foundations/03-polynomials.html",
    "title": "3  Polynomials",
    "section": "",
    "text": "3.1 Polynomial Spaces\nIn linear algebra, we often work with polynomials of bounded degree.",
    "crumbs": [
      "Preliminary",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Polynomials</span>"
    ]
  },
  {
    "objectID": "src/ch00-foundations/03-polynomials.html#polynomial-spaces",
    "href": "src/ch00-foundations/03-polynomials.html#polynomial-spaces",
    "title": "3  Polynomials",
    "section": "",
    "text": "Definition 3.4 (Polynomials of Degree at Most \\(n\\)) Let \\(F\\) be a field and \\(n \\in \\nN\\). The set of all polynomials over \\(F\\) of degree at most \\(n\\) is denoted: \\[\n    F[x]_{\\leq n} = \\left\\{ p(x) \\in F[x] : \\deg p(x) \\leq n \\right\\}\n\\] Equivalently: \\[\n    F[x]_{\\leq n} = \\left\\{ a_n x^n + a_{n-1} x^{n-1} + \\cdots + a_1 x + a_0 : a_k \\in F \\right\\}\n\\]\n\n\nExample 3.2 (Polynomials of Degree at Most 2) The set \\(\\nR[x]_{\\leq 2}\\) consists of all polynomials of degree at most \\(2\\): \\[\n    \\nR[x]_{\\leq 2} = \\{ ax^2 + bx + c : a, b, c \\in \\nR \\}\n\\] Examples include \\(3x^2 - 2x + 1\\), \\(5x + 7\\), and \\(-4\\).\n\n\nDefinition 3.5 (Polynomials of Degree Exactly \\(n\\)) We use \\(F[x]_{=n}\\) to denote polynomials of degree exactly \\(n\\): \\[\n    F[x]_{=n} = \\left\\{ p(x) \\in F[x] : \\deg p(x) = n \\right\\}\n\\] Equivalently: \\[\n    F[x]_{=n} = \\left\\{ a_n x^n + a_{n-1} x^{n-1} + \\cdots + a_1 x + a_0 : a_n \\neq 0, \\, a_k \\in F \\right\\}\n\\]\n\n\nRemark. Note the distinction:\n\n\\(F[x]_{\\leq n}\\) contains polynomials of degree \\(\\leq n\\) (including the zero polynomial)\n\\(F[x]_{=n}\\) contains polynomials of degree exactly \\(n\\) (so \\(a_n \\neq 0\\))\n\nThus \\(F[x]_{\\leq n} \\supsetneq F[x]_{=n}\\), and in fact \\(F[x]_{\\leq n} = F[x]_{=n} \\cup F[x]_{\\leq n-1}\\).\n\n\nExample 3.3 (Comparing Notations)  \n\n\\(\\nR[x]_{\\leq 2} = \\{ ax^2 + bx + c : a, b, c \\in \\nR \\}\\) includes \\(x^2 + 1\\), \\(3x - 2\\), and \\(5\\)\n\\(\\nR[x]_{=2} = \\{ ax^2 + bx + c : a \\neq 0, \\, a, b, c \\in \\nR \\}\\) includes \\(x^2 + 1\\) but not \\(3x - 2\\) or \\(5\\)",
    "crumbs": [
      "Preliminary",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Polynomials</span>"
    ]
  },
  {
    "objectID": "src/ch00-foundations/04-fields.html",
    "href": "src/ch00-foundations/04-fields.html",
    "title": "4  Fields",
    "section": "",
    "text": "$$\n\\def\\nR{\\mathbb{R}}\n\\def\\nC{\\mathbb{C}}\n\\def\\nN{\\mathbb{N}}\n\\def\\nZ{\\mathbb{Z}}\n\\def\\nQ{\\mathbb{Q}}\n\\def\\nF{\\mathbb{F}}\n\\def\\nP{\\mathbb{P}}\n\\def\\nE{\\mathbb{E}}\n\\def\\sA{\\mathcal{A}}\n\\def\\sB{\\mathcal{B}}\n\\def\\sC{\\mathcal{C}}\n\\def\\sD{\\mathcal{D}}\n\\def\\sE{\\mathcal{E}}\n\\def\\sF{\\mathcal{F}}\n\\def\\sG{\\mathcal{G}}\n\\def\\sH{\\mathcal{H}}\n\\def\\sL{\\mathcal{L}}\n\\def\\sM{\\mathcal{M}}\n\\def\\sN{\\mathcal{N}}\n\\def\\sO{\\mathcal{O}}\n\\def\\sP{\\mathcal{P}}\n\\def\\sS{\\mathcal{S}}\n\\def\\sT{\\mathcal{T}}\n\\def\\sX{\\mathcal{X}}\n\\def\\sY{\\mathcal{Y}}\n\\def\\va{\\mathbf{a}}\n\\def\\vb{\\mathbf{b}}\n\\def\\vc{\\mathbf{c}}\n\\def\\vd{\\mathbf{d}}\n\\def\\ve{\\mathbf{e}}\n\\def\\vf{\\mathbf{f}}\n\\def\\vg{\\mathbf{g}}\n\\def\\vh{\\mathbf{h}}\n\\def\\vi{\\mathbf{i}}\n\\def\\vj{\\mathbf{j}}\n\\def\\vk{\\mathbf{k}}\n\\def\\vl{\\mathbf{l}}\n\\def\\vm{\\mathbf{m}}\n\\def\\vn{\\mathbf{n}}\n\\def\\vo{\\mathbf{o}}\n\\def\\vp{\\mathbf{p}}\n\\def\\vq{\\mathbf{q}}\n\\def\\vr{\\mathbf{r}}\n\\def\\vs{\\mathbf{s}}\n\\def\\vt{\\mathbf{t}}\n\\def\\vu{\\mathbf{u}}\n\\def\\vv{\\mathbf{v}}\n\\def\\vw{\\mathbf{w}}\n\\def\\vx{\\mathbf{x}}\n\\def\\vy{\\mathbf{y}}\n\\def\\vz{\\mathbf{z}}\n\\def\\mA{\\mathbf{A}}\n\\def\\mB{\\mathbf{B}}\n\\def\\mC{\\mathbf{C}}\n\\def\\mD{\\mathbf{D}}\n\\def\\mE{\\mathbf{E}}\n\\def\\mF{\\mathbf{F}}\n\\def\\mG{\\mathbf{G}}\n\\def\\mH{\\mathbf{H}}\n\\def\\mI{\\mathbf{I}}\n\\def\\mJ{\\mathbf{J}}\n\\def\\mK{\\mathbf{K}}\n\\def\\mL{\\mathbf{L}}\n\\def\\mM{\\mathbf{M}}\n\\def\\mN{\\mathbf{N}}\n\\def\\mO{\\mathbf{O}}\n\\def\\mP{\\mathbf{P}}\n\\def\\mQ{\\mathbf{Q}}\n\\def\\mR{\\mathbf{R}}\n\\def\\mS{\\mathbf{S}}\n\\def\\mT{\\mathbf{T}}\n\\def\\mU{\\mathbf{U}}\n\\def\\mV{\\mathbf{V}}\n\\def\\mW{\\mathbf{W}}\n\\def\\mX{\\mathbf{X}}\n\\def\\mY{\\mathbf{Y}}\n\\def\\mZ{\\mathbf{Z}}\n\\def\\A{\\mathbf{A}}\n\\def\\B{\\mathbf{B}}\n\\def\\C{\\mathbf{C}}\n\\def\\D{\\mathbf{D}}\n\\def\\E{\\mathbf{E}}\n\\def\\F{\\mathbf{F}}\n\\def\\G{\\mathbf{G}}\n\\def\\H{\\mathbf{H}}\n\\def\\I{\\mathbf{I}}\n\\def\\J{\\mathbf{J}}\n\\def\\K{\\mathbf{K}}\n\\def\\L{\\mathbf{L}}\n\\def\\M{\\mathbf{M}}\n\\def\\N{\\mathbf{N}}\n\\def\\O{\\mathbf{O}}\n\\def\\P{\\mathbf{P}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\R{\\mathbf{R}}\n\\def\\S{\\mathbf{S}}\n\\def\\T{^\\top}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\W{\\mathbf{W}}\n\\def\\X{\\mathbf{X}}\n\\def\\Y{\\mathbf{Y}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\valpha{\\boldsymbol{\\alpha}}\n\\def\\vbeta{\\boldsymbol{\\beta}}\n\\def\\vgamma{\\boldsymbol{\\gamma}}\n\\def\\vdelta{\\boldsymbol{\\delta}}\n\\def\\vepsilon{\\boldsymbol{\\epsilon}}\n\\def\\vzeta{\\boldsymbol{\\zeta}}\n\\def\\veta{\\boldsymbol{\\eta}}\n\\def\\vtheta{\\boldsymbol{\\theta}}\n\\def\\viota{\\boldsymbol{\\iota}}\n\\def\\vkappa{\\boldsymbol{\\kappa}}\n\\def\\vlambda{\\boldsymbol{\\lambda}}\n\\def\\vmu{\\boldsymbol{\\mu}}\n\\def\\vnu{\\boldsymbol{\\nu}}\n\\def\\vxi{\\boldsymbol{\\xi}}\n\\def\\vpi{\\boldsymbol{\\pi}}\n\\def\\vrho{\\boldsymbol{\\rho}}\n\\def\\vsigma{\\boldsymbol{\\sigma}}\n\\def\\vtau{\\boldsymbol{\\tau}}\n\\def\\vupsilon{\\boldsymbol{\\upsilon}}\n\\def\\vphi{\\boldsymbol{\\phi}}\n\\def\\vchi{\\boldsymbol{\\chi}}\n\\def\\vpsi{\\boldsymbol{\\psi}}\n\\def\\vomega{\\boldsymbol{\\omega}}\n\\def\\vGamma{\\boldsymbol{\\Gamma}}\n\\def\\vDelta{\\boldsymbol{\\Delta}}\n\\def\\vTheta{\\boldsymbol{\\Theta}}\n\\def\\vLambda{\\boldsymbol{\\Lambda}}\n\\def\\vXi{\\boldsymbol{\\Xi}}\n\\def\\vPi{\\boldsymbol{\\Pi}}\n\\def\\vSigma{\\boldsymbol{\\Sigma}}\n\\def\\vUpsilon{\\boldsymbol{\\Upsilon}}\n\\def\\vPhi{\\boldsymbol{\\Phi}}\n\\def\\vPsi{\\boldsymbol{\\Psi}}\n\\def\\vOmega{\\boldsymbol{\\Omega}}\n\\def\\ra{\\mathbf{a}}\n\\def\\rb{\\mathbf{b}}\n\\def\\rc{\\mathbf{c}}\n\\def\\rd{\\mathbf{d}}\n\\def\\re{\\mathbf{e}}\n\\def\\rf{\\mathbf{f}}\n\\def\\rg{\\mathbf{g}}\n\\def\\rh{\\mathbf{h}}\n\\def\\ri{\\mathbf{i}}\n\\def\\rj{\\mathbf{j}}\n\\def\\rk{\\mathbf{k}}\n\\def\\rl{\\mathbf{l}}\n\\def\\rm{\\mathbf{m}}\n\\def\\rn{\\mathbf{n}}\n\\def\\ro{\\mathbf{o}}\n\\def\\rp{\\mathbf{p}}\n\\def\\rq{\\mathbf{q}}\n\\def\\rr{\\mathbf{r}}\n\\def\\rs{\\mathbf{s}}\n\\def\\rt{\\mathbf{t}}\n\\def\\ru{\\mathbf{u}}\n\\def\\rv{\\mathbf{v}}\n\\def\\rw{\\mathbf{w}}\n\\def\\rx{\\mathbf{x}}\n\\def\\ry{\\mathbf{y}}\n\\def\\rz{\\mathbf{z}}\n\\def\\vzero{\\mathbf{0}}\n\\def\\vone{\\mathbf{1}}\n\\def\\Ev{\\mathbb{E}}\n\\def\\Pr{\\operatorname{Pr}}\n\\def\\Var{\\operatorname{Var}}\n\\def\\Cov{\\operatorname{Cov}}\n\\def\\Cor{\\operatorname{Cor}}\n\\def\\Corr{\\operatorname{Corr}}\n\\def\\distBern{\\operatorname{Ber}}\n\\def\\distBin{\\operatorname{Bin}}\n\\def\\distGeom{\\operatorname{Geo}}\n\\def\\distNB{\\operatorname{NB}}\n\\def\\distPois{\\operatorname{Poi}}\n\\def\\distHyp{\\operatorname{Hyp}}\n\\def\\distUnif{\\operatorname{Unif}}\n\\def\\distExp{\\operatorname{Exp}}\n\\def\\distGamma{\\operatorname{Gamma}}\n\\def\\distN{\\mathcal{N}}\n\\def\\distBeta{\\operatorname{Beta}}\n\\def\\distCauchy{\\operatorname{Cauchy}}\n\\def\\distDir{\\operatorname{Dir}}\n\\def\\distMult{\\operatorname{Mult}}\n\\def\\distWishart{\\operatorname{Wishart}}\n\\def\\tr{\\operatorname{tr}}\n\\def\\trace{\\operatorname{tr}}\n\\def\\rank{\\operatorname{rank}}\n\\def\\nullity{\\operatorname{nullity}}\n\\def\\Span{\\operatorname{span}}\n\\def\\spn{\\operatorname{span}}\n\\def\\col{\\operatorname{col}}\n\\def\\row{\\operatorname{row}}\n\\def\\nul{\\operatorname{nul}}\n\\def\\img{\\operatorname{img}}\n\\def\\diag{\\operatorname{diag}}\n\\def\\dd{\\mathrm{d}}\n\\def\\transpose{^\\top}\n\\def\\inv{^{-1}}\n\\def\\abs#1{\\left\\lvert#1\\right\\rvert}\n\\def\\norm#1{\\left\\lVert#1\\right\\rVert}\n\\def\\floor#1{\\left\\lfloor#1\\right\\rfloor}\n\\def\\ceil#1{\\left\\lceil#1\\right\\rceil}\n\\def\\inner#1#2{\\langle #1, #2 \\rangle}\n\\def\\lcm{\\operatorname{lcm}}\n\\def\\im{\\operatorname{im}}\n\\def\\sgn{\\operatorname{sgn}}\n\\def\\argmax{\\operatorname{argmax}}\n\\def\\argmin{\\operatorname{argmin}}\n\\def\\softmax{\\operatorname{softmax}}\n\\def\\x{\\vx}\n\\def\\y{\\vy}\n\\def\\z{\\vz}\n\\def\\w{\\vw}\n\\def\\u{\\vu}\n\\def\\e{\\ve}\n\\def\\ba{\\mathbf{a}}\n\\def\\bb{\\mathbf{b}}\n\\def\\bc{\\mathbf{c}}\n\\def\\bd{\\mathbf{d}}\n\\def\\be{\\mathbf{e}}\n\\def\\bff{\\mathbf{f}}\n\\def\\bg{\\mathbf{g}}\n\\def\\bh{\\mathbf{h}}\n\\def\\bi{\\mathbf{i}}\n\\def\\bj{\\mathbf{j}}\n\\def\\bk{\\mathbf{k}}\n\\def\\bl{\\mathbf{l}}\n\\def\\bm{\\mathbf{m}}\n\\def\\bn{\\mathbf{n}}\n\\def\\bo{\\mathbf{o}}\n\\def\\bp{\\mathbf{p}}\n\\def\\bq{\\mathbf{q}}\n\\def\\br{\\mathbf{r}}\n\\def\\bs{\\mathbf{s}}\n\\def\\bt{\\mathbf{t}}\n\\def\\bu{\\mathbf{u}}\n\\def\\bv{\\mathbf{v}}\n\\def\\bw{\\mathbf{w}}\n\\def\\bx{\\mathbf{x}}\n\\def\\by{\\mathbf{y}}\n\\def\\bz{\\mathbf{z}}\n\\def\\bA{\\mathbf{A}}\n\\def\\bB{\\mathbf{B}}\n\\def\\bC{\\mathbf{C}}\n\\def\\bD{\\mathbf{D}}\n\\def\\bE{\\mathbf{E}}\n\\def\\bF{\\mathbf{F}}\n\\def\\bG{\\mathbf{G}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bJ{\\mathbf{J}}\n\\def\\bK{\\mathbf{K}}\n\\def\\bL{\\mathbf{L}}\n\\def\\bM{\\mathbf{M}}\n\\def\\bN{\\mathbf{N}}\n\\def\\bO{\\mathbf{O}}\n\\def\\bP{\\mathbf{P}}\n\\def\\bQ{\\mathbf{Q}}\n\\def\\bR{\\mathbf{R}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bT{\\mathbf{T}}\n\\def\\bU{\\mathbf{U}}\n\\def\\bV{\\mathbf{V}}\n\\def\\bW{\\mathbf{W}}\n\\def\\bX{\\mathbf{X}}\n\\def\\bY{\\mathbf{Y}}\n\\def\\bZ{\\mathbf{Z}}\n\\def\\bSigma{\\mathbf{\\Sigma}}\n\\def\\bLambda{\\mathbf{\\Lambda}}\n\\def\\bOmega{\\mathbf{\\Omega}}\n$$\n\n\nDefinition 4.1 (Field) A set \\(F\\) is called a field if we can define two binary operations addition \\(+\\), and multiplication \\(\\cdot\\) \\[\n    + : F \\times F \\to F, \\quad  \\cdot : F \\times F \\to F\n\\] such that:\n(F1) \\(a + b = b + a\\)\n(F2) \\((a + b) + c = a + (b + c)\\)\n(F3) There exists \\(0 \\in F\\) such that \\(a + 0 = a\\).\n(F4) For every \\(a \\in F\\), there exists \\(-a \\in F\\) such that \\(a + (-a) = 0\\).\n(F5) \\((a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)\\)\n(F6) \\(a \\cdot b = b \\cdot a\\)\n(F7) There exists \\(1 \\in F\\) such that \\(1 \\neq 0\\) and \\(a \\cdot 1 = a\\).\n(F8) For every \\(a \\in F \\setminus \\{0\\}\\), there exists \\(a^{-1} \\in F\\) such that \\(a \\cdot a^{-1} = 1\\).\n(F9) \\(a \\cdot (b + c) = (a \\cdot b) + (a \\cdot c)\\)\n\n\nRemark. We can shorten “\\(F\\) is a field with operations \\(+\\) and \\(\\cdot\\)” to “\\((F, +, \\cdot)\\) is a field”. Moreover when the context of \\(+\\) and \\(\\cdot\\) is clear, we simply say “\\(F\\) is a field”, which we will do most of the time in this book.\n\n\nExample 4.1 (Examples of Fields)  \n\n\\(\\nR\\) is a field.\n\\(\\nC\\) is a field.\n\\(\\nQ = \\left\\{ \\frac{p}{q} : p, q \\in \\nZ,\\; \\gcd(p, q) = 1 \\right\\}\\) is a field.\n\\(\\nZ\\) is not a field since (F8) fails.\n\\(M_n(\\nR)\\) is not a field since (F6) fails.\n\n\n\nExample 4.2 (Finite Fields) Let \\(\\nF_n\\) be the finite field with \\(n\\) elements. Then \\((\\nF_2, +, \\cdot)\\) is defined to be: \\[\n    F_2 = \\left\\{ 0, 1 \\right\\}, \\quad \\begin{array}{c|cc}\n        + & 0 & 1 \\\\\n        \\hline\n        0 & 0 & 1 \\\\\n        1 & 1 & 0\n    \\end{array}, \\quad \\begin{array}{c|cc}\n    \\cdot & 0 & 1 \\\\\n    \\hline\n    0 & 0 & 0 \\\\\n    1 & 0 & 1\n    \\end{array}.\n\\]\nThen one can verify that \\((\\nF_2, +, \\cdot)\\) is a field.",
    "crumbs": [
      "Preliminary",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fields</span>"
    ]
  },
  {
    "objectID": "src/ch00-foundations/05-matrices.html",
    "href": "src/ch00-foundations/05-matrices.html",
    "title": "5  Matrices",
    "section": "",
    "text": "5.1 Special Matrices",
    "crumbs": [
      "Preliminary",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "src/ch00-foundations/05-matrices.html#special-matrices",
    "href": "src/ch00-foundations/05-matrices.html#special-matrices",
    "title": "5  Matrices",
    "section": "",
    "text": "Definition 5.2 (Zero Matrix) The zero matrix \\(\\O \\in M_{m \\times n}(F)\\) is the matrix with all entries equal to zero: \\[\n    \\O = \\begin{bmatrix}\n        0 & 0 & \\cdots & 0 \\\\\n        0 & 0 & \\cdots & 0 \\\\\n        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        0 & 0 & \\cdots & 0\n    \\end{bmatrix}.\n\\]\n\n\nDefinition 5.3 (Identity Matrix) The identity matrix \\(\\I_n \\in M_n(F)\\) is the \\(n \\times n\\) square matrix with \\(1\\)s on the diagonal and \\(0\\)s elsewhere: \\[\n    \\I_n = \\begin{bmatrix}\n        1 & 0 & \\cdots & 0 \\\\\n        0 & 1 & \\cdots & 0 \\\\\n        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        0 & 0 & \\cdots & 1\n    \\end{bmatrix}.\n\\] When the size is clear from context, we simply write \\(\\I\\).\n\n\nDefinition 5.4 (Transpose) Let \\(\\A = (a_{ij}) \\in M_{m \\times n}(F)\\). The transpose of \\(\\A\\), denoted \\(\\A\\T\\), is the \\(n \\times m\\) matrix obtained by swapping rows and columns: \\[\n    (\\A\\T)_{ij} = a_{ji}.\n\\] Visually, the rows of \\(\\A\\) become the columns of \\(\\A\\T\\): \\[\n    \\begin{bmatrix} a & b & c \\\\ d & e & f \\end{bmatrix}\\T = \\begin{bmatrix} a & d \\\\ b & e \\\\ c & f \\end{bmatrix}.\n\\]\n\n\nDefinition 5.5 (Symmetric Matrix) A square matrix \\(\\A \\in M_n(F)\\) is symmetric if \\(\\A = \\A\\T\\). This means it is symmetric across its main diagonal: \\[\n    \\A = \\begin{bmatrix}\n        a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n        a_{12} & a_{22} & \\cdots & a_{2n} \\\\\n        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        a_{1n} & a_{2n} & \\cdots & a_{nn}\n    \\end{bmatrix}.\n\\]\n\n\nDefinition 5.6 (Diagonal Matrix) A square matrix \\(\\A \\in M_n(F)\\) is diagonal if all entries off the main diagonal are zero: \\[\n    \\A = \\begin{bmatrix}\n        d_1 & 0 & \\cdots & 0 \\\\\n        0 & d_2 & \\cdots & 0 \\\\\n        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        0 & 0 & \\cdots & d_n\n    \\end{bmatrix}.\n\\] We often write \\(\\A = \\diag(d_1, d_2, \\ldots, d_n)\\).\n\n\nDefinition 5.7 (Upper Triangular Matrix) A square matrix \\(\\A \\in M_n(F)\\) is upper triangular if all entries below the main diagonal are zero: \\[\n    \\A = \\begin{bmatrix}\n        a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n        0 & a_{22} & \\cdots & a_{2n} \\\\\n        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        0 & 0 & \\cdots & a_{nn}\n    \\end{bmatrix}.\n\\]\n\n\nExample 5.2 (Zero and Identity Matrices) \\[\n    \\O_{2 \\times 3} = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}, \\quad\n    \\I_2 = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}, \\quad\n    \\I_3 = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}.\n\\]\n\n\nExample 5.3 (Examples of Special Matrices)  \n\nTranspose: \\(\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\\T = \\begin{bmatrix} 1 & 4 \\\\ 2 & 5 \\\\ 3 & 6 \\end{bmatrix}\\).\nSymmetric: \\(\\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 5 \\\\ 3 & 5 & 6 \\end{bmatrix}\\) is symmetric since \\(a_{ij} = a_{ji}\\).\nDiagonal: \\(\\diag(3, -1, 0) = \\begin{bmatrix} 3 & 0 & 0 \\\\ 0 & -1 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}\\).\nUpper Triangular: \\(\\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 4 & 5 \\\\ 0 & 0 & 6 \\end{bmatrix}\\).",
    "crumbs": [
      "Preliminary",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "src/ch00-foundations/05-matrices.html#matrix-addition",
    "href": "src/ch00-foundations/05-matrices.html#matrix-addition",
    "title": "5  Matrices",
    "section": "5.2 Matrix Addition",
    "text": "5.2 Matrix Addition\n\nDefinition 5.8 (Matrix Addition) Let \\(\\A = (a_{ij})\\) and \\(\\B = (b_{ij})\\) be matrices in \\(M_{m \\times n}(F)\\). Their sum \\(\\A + \\B\\) is the matrix \\(\\C = (c_{ij}) \\in M_{m \\times n}(F)\\) where: \\[\n    c_{ij} = a_{ij} + b_{ij}.\n\\]\n\n\nDefinition 5.9 (Additive Inverse) The additive inverse (or negation) of a matrix \\(\\A = (a_{ij}) \\in M_{m \\times n}(F)\\) is the matrix \\(-\\A = (-a_{ij}) \\in M_{m \\times n}(F)\\).\n\n\nExample 5.4 (Matrix Addition) Let \\(\\A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\) and \\(\\B = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\\). Then: \\[\n    \\A + \\B = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} + \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\n    = \\begin{bmatrix} 1+5 & 2+6 \\\\ 3+7 & 4+8 \\end{bmatrix}\n    = \\begin{bmatrix} 6 & 8 \\\\ 10 & 12 \\end{bmatrix}.\n\\]\n\n\nTheorem 5.1 (Properties of Matrix Addition) Let \\(\\A, \\B, \\C \\in M_{m \\times n}(F)\\). Then:\n\n\\(\\A + \\B = \\B + \\A\\) (Commutativity)\n\\((\\A + \\B) + \\C = \\A + (\\B + \\C)\\) (Associativity)\n\\(\\A + \\O = \\A\\) (Additive identity)\n\\(\\A + (-\\A) = \\O\\) (Additive inverse)\n\n\n\nProof. We prove commutativity. Let \\(\\A = (a_{ij})\\) and \\(\\B = (b_{ij})\\). Then: \\[\n    (\\A + \\B)_{ij} = a_{ij} + b_{ij} = b_{ij} + a_{ij} = (\\B + \\A)_{ij}\n\\] where the second equality uses commutativity of addition in the underlying field. Since this holds for all \\(i, j\\), we have \\(\\A + \\B = \\B + \\A\\).\nThe remaining properties follow similarly.",
    "crumbs": [
      "Preliminary",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "src/ch00-foundations/05-matrices.html#matrix-multiplication",
    "href": "src/ch00-foundations/05-matrices.html#matrix-multiplication",
    "title": "5  Matrices",
    "section": "5.3 Matrix Multiplication",
    "text": "5.3 Matrix Multiplication\n\nDefinition 5.10 (Matrix Multiplication) Let \\(\\A = (a_{ij}) \\in M_{m \\times n}(F)\\) and \\(\\B = (b_{jk}) \\in M_{n \\times p}(F)\\). Their product \\(\\A\\B\\) is the matrix \\(\\C = (c_{ik}) \\in M_{m \\times p}(F)\\) where: \\[\n    c_{ik} = \\sum_{j=1}^{n} a_{ij} b_{jk}\n\\] In other words, the \\((i, k)\\)-entry of \\(\\A\\B\\) is the dot product of the \\(i\\)-th row of \\(\\A\\) with the \\(k\\)-th column of \\(\\B\\).\n\n\nExample 5.5 (Matrix Multiplication) Let \\(\\A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\in M_2(\\nR)\\) and \\(\\B = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\in M_2(\\nR)\\). We compute \\(\\A\\B\\) entry by entry: \\[\\begin{align*}\n    (\\A\\B)_{11} &= (1)(5) + (2)(7) = 5 + 14 = 19 \\\\\n    (\\A\\B)_{12} &= (1)(6) + (2)(8) = 6 + 16 = 22 \\\\\n    (\\A\\B)_{21} &= (3)(5) + (4)(7) = 15 + 28 = 43 \\\\\n    (\\A\\B)_{22} &= (3)(6) + (4)(8) = 18 + 32 = 50\n\\end{align*}\\] Therefore: \\[\n    \\A\\B = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} = \\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}\n\\]\n\n\nExample 5.6 (Matrix Multiplication with Different Dimensions) Let \\(\\A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} \\in M_{2 \\times 3}(\\nR)\\) and \\(\\B = \\begin{bmatrix} 1 \\\\ 0 \\\\ -1 \\end{bmatrix} \\in M_{3 \\times 1}(\\nR)\\). The product \\(\\A\\B \\in M_{2 \\times 1}(\\nR)\\): \\[\n    \\A\\B = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\\\ -1 \\end{bmatrix}\n    = \\begin{bmatrix} (1)(1) + (2)(0) + (3)(-1) \\\\ (4)(1) + (5)(0) + (6)(-1) \\end{bmatrix}\n    = \\begin{bmatrix} 1 + 0 - 3 \\\\ 4 + 0 - 6 \\end{bmatrix}\n    = \\begin{bmatrix} -2 \\\\ -2 \\end{bmatrix}\n\\] Note that \\(\\B\\A\\) is not defined since the number of columns of \\(\\B\\) (which is 1) does not equal the number of rows of \\(\\A\\) (which is 2).\n\n\nTheorem 5.2 (Properties of Matrix Multiplication) Let \\(\\A, \\B, \\C\\) be matrices of compatible dimensions. Then:\n\n\\((\\A\\B)\\C = \\A(\\B\\C)\\) (Associativity)\n\\(\\A(\\B + \\C) = \\A\\B + \\A\\C\\) (Left distributivity)\n\\((\\A + \\B)\\C = \\A\\C + \\B\\C\\) (Right distributivity)\n\\(\\A\\I = \\I\\A = \\A\\) (Multiplicative identity)\n\n\n\nProof. We prove associativity. Let \\(\\A\\) be \\(m \\times n\\), \\(\\B\\) be \\(n \\times p\\), and \\(\\C\\) be \\(p \\times q\\). For any entry \\((i, \\ell)\\): \\[\\begin{align*}\n    ((\\A\\B)\\C)_{i\\ell} &= \\sum_{k=1}^{p} (\\A\\B)_{ik} c_{k\\ell} = \\sum_{k=1}^{p} \\left( \\sum_{j=1}^{n} a_{ij} b_{jk} \\right) c_{k\\ell} \\\\\n    &= \\sum_{k=1}^{p} \\sum_{j=1}^{n} a_{ij} b_{jk} c_{k\\ell} = \\sum_{j=1}^{n} \\sum_{k=1}^{p} a_{ij} b_{jk} c_{k\\ell} \\\\\n    &= \\sum_{j=1}^{n} a_{ij} \\left( \\sum_{k=1}^{p} b_{jk} c_{k\\ell} \\right) = \\sum_{j=1}^{n} a_{ij} (\\B\\C)_{j\\ell} \\\\\n    &= (\\A(\\B\\C))_{i\\ell}\n\\end{align*}\\]\n\n\nRemark. Matrix multiplication is not commutative in general. That is, \\(\\A\\B \\neq \\B\\A\\) even when both products are defined.",
    "crumbs": [
      "Preliminary",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "src/ch00-foundations/05-matrices.html#scalar-multiplication",
    "href": "src/ch00-foundations/05-matrices.html#scalar-multiplication",
    "title": "5  Matrices",
    "section": "5.4 Scalar Multiplication",
    "text": "5.4 Scalar Multiplication\n\nDefinition 5.11 (Scalar Multiplication) Let \\(\\A = (a_{ij}) \\in M_{m \\times n}(F)\\) and \\(c \\in F\\) be a scalar. The scalar multiple \\(c\\A\\) is the matrix in \\(M_{m \\times n}(F)\\) with entries \\((ca_{ij})\\).",
    "crumbs": [
      "Preliminary",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "src/ch00-foundations/05-matrices.html#trace-of-a-matrix",
    "href": "src/ch00-foundations/05-matrices.html#trace-of-a-matrix",
    "title": "5  Matrices",
    "section": "5.5 Trace of a Matrix",
    "text": "5.5 Trace of a Matrix\n\nDefinition 5.12 (Trace) Let \\(\\A = (a_{ij}) \\in M_n(F)\\) be a square matrix. The trace of \\(\\A\\), denoted \\(\\tr(\\A)\\), is the sum of the entries on its main diagonal: \\[\n    \\tr(\\A) = \\sum_{i=1}^n a_{ii} = a_{11} + a_{22} + \\cdots + a_{nn}.\n\\]\n\n\nExample 5.7 (Trace) Let \\(\\A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\). Then \\(\\tr(\\A) = 1 + 5 + 9 = 15\\).",
    "crumbs": [
      "Preliminary",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "src/ch00-foundations/05-matrices.html#properties-of-transpose-and-trace",
    "href": "src/ch00-foundations/05-matrices.html#properties-of-transpose-and-trace",
    "title": "5  Matrices",
    "section": "5.6 Properties of Transpose and Trace",
    "text": "5.6 Properties of Transpose and Trace\n\nTheorem 5.3 (Properties of Transpose) Let \\(\\A, \\B\\) be matrices of compatible sizes and \\(c \\in F\\). Then:\n\n\\((\\A\\T)\\T = \\A\\)\n\\((\\A + \\B)\\T = \\A\\T + \\B\\T\\)\n\\((c\\A)\\T = c\\A\\T\\)\n\\((\\A\\B)\\T = \\B\\T\\A\\T\\) (Reversal rule)\n\n\n\nProof. We prove the reversal rule \\((\\A\\B)\\T = \\B\\T\\A\\T\\). Let \\(\\A \\in M_{m \\times n}(F)\\) and \\(\\B \\in M_{n \\times p}(F)\\). \\[\n    ((\\A\\B)\\T)_{ij} = (\\A\\B)_{ji} = \\sum_{k=1}^n a_{jk} b_{ki}\n\\] Now consider \\(\\B\\T\\A\\T\\). Its \\((i, j)\\)-entry is: \\[\n    (\\B\\T\\A\\T)_{ij} = \\sum_{k=1}^n (\\B\\T)_{ik} (\\A\\T)_{kj} = \\sum_{k=1}^n b_{ki} a_{jk} = \\sum_{k=1}^n a_{jk} b_{ki}\n\\] Since the entries are equal for all \\(i, j\\), we have \\((\\A\\B)\\T = \\B\\T\\A\\T\\).\n\n\nTheorem 5.4 (Properties of Trace) Let \\(\\A, \\B \\in M_n(F)\\) and \\(c \\in F\\). Then:\n\n\\(\\tr(\\A + \\B) = \\tr(\\A) + \\tr(\\B)\\)\n\\(\\tr(c\\A) = c\\tr(\\A)\\)\n\\(\\tr(\\A\\T) = \\tr(\\A)\\)\n\\(\\tr(\\A\\B) = \\tr(\\B\\A)\\) (Cyclic property)\n\n\n\nProof. We prove \\(\\tr(\\A\\B) = \\tr(\\B\\A)\\). \\[\n    \\tr(\\A\\B) = \\sum_{i=1}^n (\\A\\B)_{ii} = \\sum_{i=1}^n \\left( \\sum_{j=1}^n a_{ij} b_{ji} \\right)\n\\] Rearranging the summation: \\[\n    \\tr(\\A\\B) = \\sum_{j=1}^n \\sum_{i=1}^n b_{ji} a_{ij} = \\sum_{j=1}^n (\\B\\A)_{jj} = \\tr(\\B\\A).\n\\]",
    "crumbs": [
      "Preliminary",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "src/ch01-vector-spaces/index.html",
    "href": "src/ch01-vector-spaces/index.html",
    "title": "Vector Spaces and Dimensions",
    "section": "",
    "text": "In this chapter, we introduce the fundamental concept of a vector space and discuss its properties, including basis and dimension.",
    "crumbs": [
      "Vector Spaces and Dimensions"
    ]
  },
  {
    "objectID": "src/ch01-vector-spaces/01-vector-spaces.html",
    "href": "src/ch01-vector-spaces/01-vector-spaces.html",
    "title": "6  Vector Spaces",
    "section": "",
    "text": "$$\n\\def\\nR{\\mathbb{R}}\n\\def\\nC{\\mathbb{C}}\n\\def\\nN{\\mathbb{N}}\n\\def\\nZ{\\mathbb{Z}}\n\\def\\nQ{\\mathbb{Q}}\n\\def\\nF{\\mathbb{F}}\n\\def\\nP{\\mathbb{P}}\n\\def\\nE{\\mathbb{E}}\n\\def\\sA{\\mathcal{A}}\n\\def\\sB{\\mathcal{B}}\n\\def\\sC{\\mathcal{C}}\n\\def\\sD{\\mathcal{D}}\n\\def\\sE{\\mathcal{E}}\n\\def\\sF{\\mathcal{F}}\n\\def\\sG{\\mathcal{G}}\n\\def\\sH{\\mathcal{H}}\n\\def\\sL{\\mathcal{L}}\n\\def\\sM{\\mathcal{M}}\n\\def\\sN{\\mathcal{N}}\n\\def\\sO{\\mathcal{O}}\n\\def\\sP{\\mathcal{P}}\n\\def\\sS{\\mathcal{S}}\n\\def\\sT{\\mathcal{T}}\n\\def\\sX{\\mathcal{X}}\n\\def\\sY{\\mathcal{Y}}\n\\def\\va{\\mathbf{a}}\n\\def\\vb{\\mathbf{b}}\n\\def\\vc{\\mathbf{c}}\n\\def\\vd{\\mathbf{d}}\n\\def\\ve{\\mathbf{e}}\n\\def\\vf{\\mathbf{f}}\n\\def\\vg{\\mathbf{g}}\n\\def\\vh{\\mathbf{h}}\n\\def\\vi{\\mathbf{i}}\n\\def\\vj{\\mathbf{j}}\n\\def\\vk{\\mathbf{k}}\n\\def\\vl{\\mathbf{l}}\n\\def\\vm{\\mathbf{m}}\n\\def\\vn{\\mathbf{n}}\n\\def\\vo{\\mathbf{o}}\n\\def\\vp{\\mathbf{p}}\n\\def\\vq{\\mathbf{q}}\n\\def\\vr{\\mathbf{r}}\n\\def\\vs{\\mathbf{s}}\n\\def\\vt{\\mathbf{t}}\n\\def\\vu{\\mathbf{u}}\n\\def\\vv{\\mathbf{v}}\n\\def\\vw{\\mathbf{w}}\n\\def\\vx{\\mathbf{x}}\n\\def\\vy{\\mathbf{y}}\n\\def\\vz{\\mathbf{z}}\n\\def\\mA{\\mathbf{A}}\n\\def\\mB{\\mathbf{B}}\n\\def\\mC{\\mathbf{C}}\n\\def\\mD{\\mathbf{D}}\n\\def\\mE{\\mathbf{E}}\n\\def\\mF{\\mathbf{F}}\n\\def\\mG{\\mathbf{G}}\n\\def\\mH{\\mathbf{H}}\n\\def\\mI{\\mathbf{I}}\n\\def\\mJ{\\mathbf{J}}\n\\def\\mK{\\mathbf{K}}\n\\def\\mL{\\mathbf{L}}\n\\def\\mM{\\mathbf{M}}\n\\def\\mN{\\mathbf{N}}\n\\def\\mO{\\mathbf{O}}\n\\def\\mP{\\mathbf{P}}\n\\def\\mQ{\\mathbf{Q}}\n\\def\\mR{\\mathbf{R}}\n\\def\\mS{\\mathbf{S}}\n\\def\\mT{\\mathbf{T}}\n\\def\\mU{\\mathbf{U}}\n\\def\\mV{\\mathbf{V}}\n\\def\\mW{\\mathbf{W}}\n\\def\\mX{\\mathbf{X}}\n\\def\\mY{\\mathbf{Y}}\n\\def\\mZ{\\mathbf{Z}}\n\\def\\A{\\mathbf{A}}\n\\def\\B{\\mathbf{B}}\n\\def\\C{\\mathbf{C}}\n\\def\\D{\\mathbf{D}}\n\\def\\E{\\mathbf{E}}\n\\def\\F{\\mathbf{F}}\n\\def\\G{\\mathbf{G}}\n\\def\\H{\\mathbf{H}}\n\\def\\I{\\mathbf{I}}\n\\def\\J{\\mathbf{J}}\n\\def\\K{\\mathbf{K}}\n\\def\\L{\\mathbf{L}}\n\\def\\M{\\mathbf{M}}\n\\def\\N{\\mathbf{N}}\n\\def\\O{\\mathbf{O}}\n\\def\\P{\\mathbf{P}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\R{\\mathbf{R}}\n\\def\\S{\\mathbf{S}}\n\\def\\T{^\\top}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\W{\\mathbf{W}}\n\\def\\X{\\mathbf{X}}\n\\def\\Y{\\mathbf{Y}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\valpha{\\boldsymbol{\\alpha}}\n\\def\\vbeta{\\boldsymbol{\\beta}}\n\\def\\vgamma{\\boldsymbol{\\gamma}}\n\\def\\vdelta{\\boldsymbol{\\delta}}\n\\def\\vepsilon{\\boldsymbol{\\epsilon}}\n\\def\\vzeta{\\boldsymbol{\\zeta}}\n\\def\\veta{\\boldsymbol{\\eta}}\n\\def\\vtheta{\\boldsymbol{\\theta}}\n\\def\\viota{\\boldsymbol{\\iota}}\n\\def\\vkappa{\\boldsymbol{\\kappa}}\n\\def\\vlambda{\\boldsymbol{\\lambda}}\n\\def\\vmu{\\boldsymbol{\\mu}}\n\\def\\vnu{\\boldsymbol{\\nu}}\n\\def\\vxi{\\boldsymbol{\\xi}}\n\\def\\vpi{\\boldsymbol{\\pi}}\n\\def\\vrho{\\boldsymbol{\\rho}}\n\\def\\vsigma{\\boldsymbol{\\sigma}}\n\\def\\vtau{\\boldsymbol{\\tau}}\n\\def\\vupsilon{\\boldsymbol{\\upsilon}}\n\\def\\vphi{\\boldsymbol{\\phi}}\n\\def\\vchi{\\boldsymbol{\\chi}}\n\\def\\vpsi{\\boldsymbol{\\psi}}\n\\def\\vomega{\\boldsymbol{\\omega}}\n\\def\\vGamma{\\boldsymbol{\\Gamma}}\n\\def\\vDelta{\\boldsymbol{\\Delta}}\n\\def\\vTheta{\\boldsymbol{\\Theta}}\n\\def\\vLambda{\\boldsymbol{\\Lambda}}\n\\def\\vXi{\\boldsymbol{\\Xi}}\n\\def\\vPi{\\boldsymbol{\\Pi}}\n\\def\\vSigma{\\boldsymbol{\\Sigma}}\n\\def\\vUpsilon{\\boldsymbol{\\Upsilon}}\n\\def\\vPhi{\\boldsymbol{\\Phi}}\n\\def\\vPsi{\\boldsymbol{\\Psi}}\n\\def\\vOmega{\\boldsymbol{\\Omega}}\n\\def\\ra{\\mathbf{a}}\n\\def\\rb{\\mathbf{b}}\n\\def\\rc{\\mathbf{c}}\n\\def\\rd{\\mathbf{d}}\n\\def\\re{\\mathbf{e}}\n\\def\\rf{\\mathbf{f}}\n\\def\\rg{\\mathbf{g}}\n\\def\\rh{\\mathbf{h}}\n\\def\\ri{\\mathbf{i}}\n\\def\\rj{\\mathbf{j}}\n\\def\\rk{\\mathbf{k}}\n\\def\\rl{\\mathbf{l}}\n\\def\\rm{\\mathbf{m}}\n\\def\\rn{\\mathbf{n}}\n\\def\\ro{\\mathbf{o}}\n\\def\\rp{\\mathbf{p}}\n\\def\\rq{\\mathbf{q}}\n\\def\\rr{\\mathbf{r}}\n\\def\\rs{\\mathbf{s}}\n\\def\\rt{\\mathbf{t}}\n\\def\\ru{\\mathbf{u}}\n\\def\\rv{\\mathbf{v}}\n\\def\\rw{\\mathbf{w}}\n\\def\\rx{\\mathbf{x}}\n\\def\\ry{\\mathbf{y}}\n\\def\\rz{\\mathbf{z}}\n\\def\\vzero{\\mathbf{0}}\n\\def\\vone{\\mathbf{1}}\n\\def\\Ev{\\mathbb{E}}\n\\def\\Pr{\\operatorname{Pr}}\n\\def\\Var{\\operatorname{Var}}\n\\def\\Cov{\\operatorname{Cov}}\n\\def\\Cor{\\operatorname{Cor}}\n\\def\\Corr{\\operatorname{Corr}}\n\\def\\distBern{\\operatorname{Ber}}\n\\def\\distBin{\\operatorname{Bin}}\n\\def\\distGeom{\\operatorname{Geo}}\n\\def\\distNB{\\operatorname{NB}}\n\\def\\distPois{\\operatorname{Poi}}\n\\def\\distHyp{\\operatorname{Hyp}}\n\\def\\distUnif{\\operatorname{Unif}}\n\\def\\distExp{\\operatorname{Exp}}\n\\def\\distGamma{\\operatorname{Gamma}}\n\\def\\distN{\\mathcal{N}}\n\\def\\distBeta{\\operatorname{Beta}}\n\\def\\distCauchy{\\operatorname{Cauchy}}\n\\def\\distDir{\\operatorname{Dir}}\n\\def\\distMult{\\operatorname{Mult}}\n\\def\\distWishart{\\operatorname{Wishart}}\n\\def\\tr{\\operatorname{tr}}\n\\def\\trace{\\operatorname{tr}}\n\\def\\rank{\\operatorname{rank}}\n\\def\\nullity{\\operatorname{nullity}}\n\\def\\Span{\\operatorname{span}}\n\\def\\spn{\\operatorname{span}}\n\\def\\col{\\operatorname{col}}\n\\def\\row{\\operatorname{row}}\n\\def\\nul{\\operatorname{nul}}\n\\def\\img{\\operatorname{img}}\n\\def\\diag{\\operatorname{diag}}\n\\def\\dd{\\mathrm{d}}\n\\def\\transpose{^\\top}\n\\def\\inv{^{-1}}\n\\def\\abs#1{\\left\\lvert#1\\right\\rvert}\n\\def\\norm#1{\\left\\lVert#1\\right\\rVert}\n\\def\\floor#1{\\left\\lfloor#1\\right\\rfloor}\n\\def\\ceil#1{\\left\\lceil#1\\right\\rceil}\n\\def\\inner#1#2{\\langle #1, #2 \\rangle}\n\\def\\lcm{\\operatorname{lcm}}\n\\def\\im{\\operatorname{im}}\n\\def\\sgn{\\operatorname{sgn}}\n\\def\\argmax{\\operatorname{argmax}}\n\\def\\argmin{\\operatorname{argmin}}\n\\def\\softmax{\\operatorname{softmax}}\n\\def\\x{\\vx}\n\\def\\y{\\vy}\n\\def\\z{\\vz}\n\\def\\w{\\vw}\n\\def\\u{\\vu}\n\\def\\e{\\ve}\n\\def\\ba{\\mathbf{a}}\n\\def\\bb{\\mathbf{b}}\n\\def\\bc{\\mathbf{c}}\n\\def\\bd{\\mathbf{d}}\n\\def\\be{\\mathbf{e}}\n\\def\\bff{\\mathbf{f}}\n\\def\\bg{\\mathbf{g}}\n\\def\\bh{\\mathbf{h}}\n\\def\\bi{\\mathbf{i}}\n\\def\\bj{\\mathbf{j}}\n\\def\\bk{\\mathbf{k}}\n\\def\\bl{\\mathbf{l}}\n\\def\\bm{\\mathbf{m}}\n\\def\\bn{\\mathbf{n}}\n\\def\\bo{\\mathbf{o}}\n\\def\\bp{\\mathbf{p}}\n\\def\\bq{\\mathbf{q}}\n\\def\\br{\\mathbf{r}}\n\\def\\bs{\\mathbf{s}}\n\\def\\bt{\\mathbf{t}}\n\\def\\bu{\\mathbf{u}}\n\\def\\bv{\\mathbf{v}}\n\\def\\bw{\\mathbf{w}}\n\\def\\bx{\\mathbf{x}}\n\\def\\by{\\mathbf{y}}\n\\def\\bz{\\mathbf{z}}\n\\def\\bA{\\mathbf{A}}\n\\def\\bB{\\mathbf{B}}\n\\def\\bC{\\mathbf{C}}\n\\def\\bD{\\mathbf{D}}\n\\def\\bE{\\mathbf{E}}\n\\def\\bF{\\mathbf{F}}\n\\def\\bG{\\mathbf{G}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bJ{\\mathbf{J}}\n\\def\\bK{\\mathbf{K}}\n\\def\\bL{\\mathbf{L}}\n\\def\\bM{\\mathbf{M}}\n\\def\\bN{\\mathbf{N}}\n\\def\\bO{\\mathbf{O}}\n\\def\\bP{\\mathbf{P}}\n\\def\\bQ{\\mathbf{Q}}\n\\def\\bR{\\mathbf{R}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bT{\\mathbf{T}}\n\\def\\bU{\\mathbf{U}}\n\\def\\bV{\\mathbf{V}}\n\\def\\bW{\\mathbf{W}}\n\\def\\bX{\\mathbf{X}}\n\\def\\bY{\\mathbf{Y}}\n\\def\\bZ{\\mathbf{Z}}\n\\def\\bSigma{\\mathbf{\\Sigma}}\n\\def\\bLambda{\\mathbf{\\Lambda}}\n\\def\\bOmega{\\mathbf{\\Omega}}\n$$\n\nWhy do we bother with a list of eight axioms? To a student first encountering Linear Algebra, this can feel like a bureaucratic exercise in “checking boxes.” However, there is a profound beauty hidden in this formality.\nIn our preliminary chapter, we saw three very different worlds: the world of geometric arrows (\\(\\nR^n\\)), the world of functions (\\(F[x]\\)), and the world of matrices (\\(M_{m \\times n}\\)). On the surface, an arrow is not a polynomial, and a polynomial is not a matrix. But if you squint, they all behave the same: you can add them, and you can scale them.\nBy defining a Vector Space through these axioms, we are choosing to ignore what the objects are and focus entirely on how they act. If we prove a theorem using only these eight axioms, that theorem becomes a “universal law” that applies to arrows, functions, and matrices all at once. This is the power of abstraction: solve the problem once, and you solve it for every universe that obeys these rules.\n\nDefinition 6.1 (Vector Space) A set \\(V\\) over a field \\(F\\) with two operations: addition \\(+\\) and scalar multiplication \\(\\cdot\\) \\[\n    + : V \\times V \\to V, \\quad  \\cdot : F \\times V \\to V\n\\] such that satisfy the following axioms for all \\(\\vv_1, \\vv_2, \\vv_3 \\in V\\) and \\(\\alpha, \\beta \\in F\\):\n(VS1) \\(\\vv_1 + \\vv_2 = \\vv_2 + \\vv_1\\)\n(VS2) \\((\\vv_1 + \\vv_2) + \\vv_3 = \\vv_1 + (\\vv_2 + \\vv_3)\\)\n(VS3) There exists \\(\\vzero \\in V\\) such that \\(\\vv_1 + \\vzero = \\vv_1\\)\n(VS4) There exists \\(\\vv_1' \\in V\\) such that \\(\\vv_1 + \\vv_1' = \\vzero\\)\n(VS5) \\(\\alpha(\\vv_1 + \\vv_2) = \\alpha\\vv_1 + \\alpha\\vv_2\\)\n(VS6) \\((\\alpha+\\beta)\\vv_1 = \\alpha\\vv_1 + \\beta\\vv_1\\)\n(VS7) \\(\\alpha(\\beta\\vv_1) = (\\alpha\\beta)\\vv_1\\)\n(VS8) \\(1\\vv_1 = \\vv_1\\)\n\n\nRemark. Instead of writing \\((V, +, \\cdot)\\) is a vector space over \\(F\\), we usually simplify it to \\(V\\) is a vector space over \\(F\\), or even just \\(V\\) is a vector space if the context is clear.\n\n\nRemark. We sometimes will abuse the name and refer to \\(\\vzero = \\vzero_V \\in V\\) as the “zero vector” of the vector space \\(V\\). Readers should not confuse \\(\\vzero_V\\) with the zero scalar \\(0 = 0_F \\in F\\) from the field \\(F\\).\n\n\nExample 6.1 (Examples of Vector Spaces)  \n\nMost defaultly, \\(F^n\\) with the usual operations of \\(+\\) and \\(\\cdot\\) is a vector space over \\(F\\).\nWorking with polynomials is common too: \\(F[x]\\) with the usual operations of \\(+\\) and \\(\\cdot\\) is a vector space over \\(F\\).\nLet \\(\\sD\\) be any open interval. Let \\[\nC(\\sD) \\coloneqq \\left\\{ f: \\sD \\to \\nR: f \\text{ is continuous} \\right\\}.\n\\] Then \\(C(\\sD)\\) is a vector space over \\(\\nR\\). The zero vector of this vector space is given by the zero polynomial \\((x \\mapsto 0)\\).\nLet \\(F\\) be any field and fix \\(n \\in \\nN\\). Then \\(M_n(F)\\) is a vector space over \\(F\\).\nThe set \\(V = \\mathbb{R}_{&gt;0}\\) of positive real numbers forms a vector space over \\(F = \\mathbb{R}\\) under the following operations: for \\(x, y \\in V\\) and \\(\\alpha \\in F\\), define \\[\n     x \\oplus y \\coloneqq xy, \\quad \\alpha \\odot x \\coloneqq x^\\alpha = e^{\\alpha \\log x}.\n\\] Under these operations, the zero vector is \\(\\vzero = 1\\), and the additive inverse of \\(x\\) is \\(x^{-1}\\).\n\nYou can check if addition and scalar multiplication make sense and follow all axioms of vector spaces.\n\n\nExample 6.2 (\\(\\nQ\\) Adjoin \\(\\sqrt{2}\\)) We verify that \\(\\nQ(\\sqrt{2}) \\coloneqq \\left\\{  a + b \\sqrt{2} : a, b \\in \\nQ  \\right\\}\\) is a vector space over \\(\\nQ\\) by checking all eight axioms. Let \\(\\vv_1 = a_1 + b_1\\sqrt{2}\\), \\(\\vv_2 = a_2 + b_2\\sqrt{2}\\), \\(\\vv_3 = a_3 + b_3\\sqrt{2} \\in \\nQ(\\sqrt{2})\\) and \\(\\alpha, \\beta \\in \\nQ\\).\n(VS1) \\(\\vv_1 + \\vv_2 = (a_1 + a_2) + (b_1 + b_2)\\sqrt{2} = (a_2 + a_1) + (b_2 + b_1)\\sqrt{2} = \\vv_2 + \\vv_1\\).\n(VS2) \\((\\vv_1 + \\vv_2) + \\vv_3 = (a_1 + a_2 + a_3) + (b_1 + b_2 + b_3)\\sqrt{2} = \\vv_1 + (\\vv_2 + \\vv_3)\\).\n(VS3) The element \\(\\vzero = 0 + 0\\sqrt{2} = 0 \\in \\nQ(\\sqrt{2})\\) satisfies \\(\\vv_1 + \\vzero = (a_1 + 0) + (b_1 + 0)\\sqrt{2} = \\vv_1\\).\n(VS4) For \\(\\vv_1 = a_1 + b_1\\sqrt{2}\\), define \\(-\\vv_1 \\coloneqq (-a_1) + (-b_1)\\sqrt{2} \\in \\nQ(\\sqrt{2})\\). Then \\(\\vv_1 + (-\\vv_1) = (a_1 - a_1) + (b_1 - b_1)\\sqrt{2} = \\vzero\\).\n(VS5) \\(\\alpha(\\vv_1 + \\vv_2) = \\alpha(a_1 + a_2) + \\alpha(b_1 + b_2)\\sqrt{2} = (\\alpha a_1 + \\alpha a_2) + (\\alpha b_1 + \\alpha b_2)\\sqrt{2} = \\alpha\\vv_1 + \\alpha\\vv_2\\).\n(VS6) \\((\\alpha + \\beta)\\vv_1 = (\\alpha + \\beta)a_1 + (\\alpha + \\beta)b_1\\sqrt{2} = (\\alpha a_1 + \\beta a_1) + (\\alpha b_1 + \\beta b_1)\\sqrt{2} = \\alpha\\vv_1 + \\beta\\vv_1\\).\n(VS7) \\(\\alpha(\\beta\\vv_1) = \\alpha(\\beta a_1 + \\beta b_1\\sqrt{2}) = \\alpha\\beta a_1 + \\alpha\\beta b_1\\sqrt{2} = (\\alpha\\beta)\\vv_1\\).\n(VS8) \\(1 \\cdot \\vv_1 = 1 \\cdot a_1 + 1 \\cdot b_1\\sqrt{2} = a_1 + b_1\\sqrt{2} = \\vv_1\\).\nThus \\(\\nQ(\\sqrt{2})\\) is a vector space over \\(\\nQ\\).\n\n\nDefinition 6.2 (Vectors and Scalars) Let \\(V\\) be a vector space over \\(F\\). Then the elements of \\(V\\) are called vectors, and the elements of \\(F\\) are called scalars.\nAdditionally, \\(\\vzero \\in V\\) is called the zero vector, and \\((\\vv_1')\\) in (VS4) is called the inverse element of \\(\\vv_1\\).\n\n\nTheorem 6.1 (Left Cancellation Law) Let \\(V\\) be a vector space over \\(F\\), let \\(\\vu, \\vv_1, \\vv_2 \\in V\\). If \\(\\vu + \\vv_1 = \\vu + \\vv_2\\), then \\(\\vv_1 = \\vv_2\\).\n\n\nProof. Let \\(\\vu'\\) be an inverse of \\(\\vu\\). \\[\\begin{align*}\n\\vu + \\vv_1 &= \\vu + \\vv_2 & \\text{(given)} \\\\\n\\vu' + (\\vu + \\vv_1) &= \\vu' + (\\vu + \\vv_2) & \\text{(add } \\vu' \\text{ to both sides)} \\\\\n(\\vu' + \\vu) + \\vv_1 &= (\\vu' + \\vu) + \\vv_2 & \\text{(by VS2)} \\\\\n\\vzero + \\vv_1 &= \\vzero + \\vv_2 & \\text{(by VS4)} \\\\\n\\vv_1 &= \\vv_2 & \\text{(by VS3)}\n\\end{align*}\\]\n\n\nTheorem 6.2 (Right Cancellation Law) Let \\(V\\) be a vector space over \\(F\\), let \\(\\vu, \\vv_1, \\vv_2 \\in V\\). If \\(\\vv_1 + \\vu = \\vv_2 + \\vu\\), then \\(\\vv_1 = \\vv_2\\).\n\n\nProof. Since we have \\(\\vu + \\vv_1 = \\vu + \\vv_2 \\implies \\vv_1 = \\vv_2\\), applying (VS1) to it gives \\(\\vv_1 + \\vu = \\vv_2 + \\vu \\implies \\vv_1 = \\vv_2\\).\n\n\nTheorem 6.3 (Zero Vector is Unique) Let \\(V\\) be a vector space over \\(F\\). The zero vector \\(\\vzero \\in V\\) is unique.\n\n\nProof. Suppose that there are two vectors \\(\\vzero_1, \\vzero_2\\). \\[\\begin{align*}\n\\vzero_1 + \\vzero_2 &= \\vzero_2 + \\vzero_1 & \\text{(by VS1)} \\\\\n\\vzero_1 &= \\vzero_2 & \\text{(by VS3)}\n\\end{align*}\\]\n\n\nTheorem 6.4 (Additive Inverse is Unique) Let \\(V\\) be a vector space over \\(F\\). Then for every \\(\\vv \\in V\\), its additive inverse described in (VS4), which is \\(\\vv'\\), is unique.\n\n\nProof. Let \\(\\vv_1', \\vv_2'\\) both be inverses of \\(\\vv\\). Then \\[\\begin{align*}\n\\vv_1' &= \\vv_1' + \\vzero & \\text{(by VS3)} \\\\\n&= \\vv_1' + (\\vv + \\vv_2') & \\text{(by VS4)} \\\\\n&= (\\vv_1' + \\vv) + \\vv_2' & \\text{(by VS2)} \\\\\n&= \\vzero + \\vv_2' & \\text{(by VS4)} \\\\\n&= \\vv_2' & \\text{(by VS3)}\n\\end{align*}\\]\n\n\nDefinition 6.3 (Notation of Additive Inverse) The unique inverse of a vector \\(\\vv \\in V\\), for a vector space \\(V\\) over \\(F\\), will be denoted as \\(-\\vv\\).\n\n\nTheorem 6.5 (Zero Scalar Annihilates) Let \\(\\vv \\in V\\) for a vector space \\(V\\) over \\(F\\). Then \\(0 \\cdot \\vv = \\vzero\\).\n\n\nProof. Let \\(\\vw = 0 \\cdot \\vv\\). We show that \\(\\vw = \\vzero\\). \\[\\begin{align*}\n\\vw + \\vw &= 0 \\cdot \\vv + 0 \\cdot \\vv \\\\\n&= (0 + 0) \\cdot \\vv & \\text{(by VS6)} \\\\\n&= 0 \\cdot \\vv & \\text{(arithmetic in } F \\text{)} \\\\\n&= \\vw \\\\\n(\\vw + \\vw) + (-\\vw) &= \\vw + (-\\vw) & \\text{(add } -\\vw \\text{ to both sides)} \\\\\n\\vw + (\\vw + (-\\vw)) &= \\vzero & \\text{(by VS2, VS4)} \\\\\n\\vw + \\vzero &= \\vzero & \\text{(by VS4)} \\\\\n\\vw &= \\vzero & \\text{(by VS3)}\n\\end{align*}\\]\n\n\nTheorem 6.6 (Negation as Scalar Multiplication) Let \\(V\\) be a vector space over \\(F\\) and \\(\\vv \\in V\\) be any vector. Then \\(-\\vv = (-1) \\cdot \\vv\\).\n\n\nProof. We show that \\((-1) \\cdot \\vv\\) is an additive inverse of \\(\\vv\\): \\[\\begin{align*}\n\\vv + (-1) \\cdot \\vv &= 1 \\cdot \\vv + (-1) \\cdot \\vv & \\text{(by VS8)} \\\\\n&= (1 + (-1)) \\cdot \\vv & \\text{(by VS6)} \\\\\n&= 0 \\cdot \\vv & \\text{(arithmetic in } F \\text{)} \\\\\n&= \\vzero & \\text{(by @thm-zero-scalar-mult)}\n\\end{align*}\\] Since \\((-1) \\cdot \\vv\\) is an additive inverse of \\(\\vv\\), by uniqueness we have \\(-\\vv = (-1) \\cdot \\vv\\).\n\n\nTheorem 6.7 (Negative Scalar Distribution) Let \\(V\\) be a vector space over \\(F\\). For any \\(\\alpha \\in F\\) and \\(\\vv \\in V\\), we have \\((-\\alpha)\\vv = -(\\alpha\\vv)\\).\n\n\nProof. We show that \\((-\\alpha)\\vv\\) is an additive inverse of \\(\\alpha\\vv\\): \\[\\begin{align*}\n\\alpha\\vv + (-\\alpha)\\vv &= (\\alpha + (-\\alpha))\\vv & \\text{(by VS6)} \\\\\n&= 0 \\cdot \\vv & \\text{(arithmetic in } F \\text{)} \\\\\n&= \\vzero & \\text{(by @thm-zero-scalar-mult)}\n\\end{align*}\\] Since \\((-\\alpha)\\vv\\) is an additive inverse of \\(\\alpha\\vv\\), by uniqueness we have \\((-\\alpha)\\vv = -(\\alpha\\vv)\\).\n\n\nTheorem 6.8 (Scalar Multiplication by Zero Vector) Let \\(V\\) be a vector space over \\(F\\). For any scalar \\(\\alpha \\in F\\), we have \\(\\alpha\\vzero = \\vzero\\).\n\n\nProof. Let \\(\\vw = \\alpha\\vzero\\). We show that \\(\\vw = \\vzero\\). \\[\\begin{align*}\n\\vw &= \\alpha\\vzero \\\\\n&= \\alpha(\\vzero + \\vzero) & \\text{(by VS3)} \\\\\n&= \\alpha\\vzero + \\alpha\\vzero & \\text{(by VS5)} \\\\\n&= \\vw + \\vw \\\\\n\\vw + (-\\vw) &= (\\vw + \\vw) + (-\\vw) & \\text{(add } -\\vw \\text{ to both sides)} \\\\\n\\vzero &= \\vw + (\\vw + (-\\vw)) & \\text{(by VS4, VS2)} \\\\\n\\vzero &= \\vw + \\vzero & \\text{(by VS4)} \\\\\n\\vzero &= \\vw & \\text{(by VS3)}\n\\end{align*}\\]\n\n\nTheorem 6.9 (Zero Product Law) Let \\(V\\) be a vector space over \\(F\\), \\(\\vv \\in V\\), and \\(\\alpha \\in F\\). If \\(\\alpha\\vv = \\vzero\\), then either \\(\\alpha = 0\\) or \\(\\vv = \\vzero\\).\n\n\nProof. Suppose \\(\\alpha\\vv = \\vzero\\). We consider two cases:\n\nCase \\(\\alpha = 0\\): Then the statement holds.\nCase \\(\\alpha \\neq 0\\): Since \\(F\\) is a field, \\(\\alpha\\) has a multiplicative inverse \\(\\alpha^{-1}\\). \\[\\begin{align*}\n\\alpha\\vv &= \\vzero & \\text{(given)} \\\\\n\\alpha^{-1}(\\alpha\\vv) &= \\alpha^{-1}\\vzero & \\text{(multiply by } \\alpha^{-1} \\text{)} \\\\\n(\\alpha^{-1}\\alpha)\\vv &= \\vzero & \\text{(by VS7 and previous Theorem)} \\\\\n1 \\cdot \\vv &= \\vzero & \\text{(field inverse property)} \\\\\n\\vv &= \\vzero & \\text{(by VS8)}\n\\end{align*}\\] Thus, if \\(\\alpha \\neq 0\\), we must have \\(\\vv = \\vzero\\).",
    "crumbs": [
      "Vector Spaces and Dimensions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Vector Spaces</span>"
    ]
  },
  {
    "objectID": "src/ch01-vector-spaces/02-subspaces.html",
    "href": "src/ch01-vector-spaces/02-subspaces.html",
    "title": "7  Subspaces",
    "section": "",
    "text": "$$\n\\def\\nR{\\mathbb{R}}\n\\def\\nC{\\mathbb{C}}\n\\def\\nN{\\mathbb{N}}\n\\def\\nZ{\\mathbb{Z}}\n\\def\\nQ{\\mathbb{Q}}\n\\def\\nF{\\mathbb{F}}\n\\def\\nP{\\mathbb{P}}\n\\def\\nE{\\mathbb{E}}\n\\def\\sA{\\mathcal{A}}\n\\def\\sB{\\mathcal{B}}\n\\def\\sC{\\mathcal{C}}\n\\def\\sD{\\mathcal{D}}\n\\def\\sE{\\mathcal{E}}\n\\def\\sF{\\mathcal{F}}\n\\def\\sG{\\mathcal{G}}\n\\def\\sH{\\mathcal{H}}\n\\def\\sL{\\mathcal{L}}\n\\def\\sM{\\mathcal{M}}\n\\def\\sN{\\mathcal{N}}\n\\def\\sO{\\mathcal{O}}\n\\def\\sP{\\mathcal{P}}\n\\def\\sS{\\mathcal{S}}\n\\def\\sT{\\mathcal{T}}\n\\def\\sX{\\mathcal{X}}\n\\def\\sY{\\mathcal{Y}}\n\\def\\va{\\mathbf{a}}\n\\def\\vb{\\mathbf{b}}\n\\def\\vc{\\mathbf{c}}\n\\def\\vd{\\mathbf{d}}\n\\def\\ve{\\mathbf{e}}\n\\def\\vf{\\mathbf{f}}\n\\def\\vg{\\mathbf{g}}\n\\def\\vh{\\mathbf{h}}\n\\def\\vi{\\mathbf{i}}\n\\def\\vj{\\mathbf{j}}\n\\def\\vk{\\mathbf{k}}\n\\def\\vl{\\mathbf{l}}\n\\def\\vm{\\mathbf{m}}\n\\def\\vn{\\mathbf{n}}\n\\def\\vo{\\mathbf{o}}\n\\def\\vp{\\mathbf{p}}\n\\def\\vq{\\mathbf{q}}\n\\def\\vr{\\mathbf{r}}\n\\def\\vs{\\mathbf{s}}\n\\def\\vt{\\mathbf{t}}\n\\def\\vu{\\mathbf{u}}\n\\def\\vv{\\mathbf{v}}\n\\def\\vw{\\mathbf{w}}\n\\def\\vx{\\mathbf{x}}\n\\def\\vy{\\mathbf{y}}\n\\def\\vz{\\mathbf{z}}\n\\def\\mA{\\mathbf{A}}\n\\def\\mB{\\mathbf{B}}\n\\def\\mC{\\mathbf{C}}\n\\def\\mD{\\mathbf{D}}\n\\def\\mE{\\mathbf{E}}\n\\def\\mF{\\mathbf{F}}\n\\def\\mG{\\mathbf{G}}\n\\def\\mH{\\mathbf{H}}\n\\def\\mI{\\mathbf{I}}\n\\def\\mJ{\\mathbf{J}}\n\\def\\mK{\\mathbf{K}}\n\\def\\mL{\\mathbf{L}}\n\\def\\mM{\\mathbf{M}}\n\\def\\mN{\\mathbf{N}}\n\\def\\mO{\\mathbf{O}}\n\\def\\mP{\\mathbf{P}}\n\\def\\mQ{\\mathbf{Q}}\n\\def\\mR{\\mathbf{R}}\n\\def\\mS{\\mathbf{S}}\n\\def\\mT{\\mathbf{T}}\n\\def\\mU{\\mathbf{U}}\n\\def\\mV{\\mathbf{V}}\n\\def\\mW{\\mathbf{W}}\n\\def\\mX{\\mathbf{X}}\n\\def\\mY{\\mathbf{Y}}\n\\def\\mZ{\\mathbf{Z}}\n\\def\\A{\\mathbf{A}}\n\\def\\B{\\mathbf{B}}\n\\def\\C{\\mathbf{C}}\n\\def\\D{\\mathbf{D}}\n\\def\\E{\\mathbf{E}}\n\\def\\F{\\mathbf{F}}\n\\def\\G{\\mathbf{G}}\n\\def\\H{\\mathbf{H}}\n\\def\\I{\\mathbf{I}}\n\\def\\J{\\mathbf{J}}\n\\def\\K{\\mathbf{K}}\n\\def\\L{\\mathbf{L}}\n\\def\\M{\\mathbf{M}}\n\\def\\N{\\mathbf{N}}\n\\def\\O{\\mathbf{O}}\n\\def\\P{\\mathbf{P}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\R{\\mathbf{R}}\n\\def\\S{\\mathbf{S}}\n\\def\\T{^\\top}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\W{\\mathbf{W}}\n\\def\\X{\\mathbf{X}}\n\\def\\Y{\\mathbf{Y}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\valpha{\\boldsymbol{\\alpha}}\n\\def\\vbeta{\\boldsymbol{\\beta}}\n\\def\\vgamma{\\boldsymbol{\\gamma}}\n\\def\\vdelta{\\boldsymbol{\\delta}}\n\\def\\vepsilon{\\boldsymbol{\\epsilon}}\n\\def\\vzeta{\\boldsymbol{\\zeta}}\n\\def\\veta{\\boldsymbol{\\eta}}\n\\def\\vtheta{\\boldsymbol{\\theta}}\n\\def\\viota{\\boldsymbol{\\iota}}\n\\def\\vkappa{\\boldsymbol{\\kappa}}\n\\def\\vlambda{\\boldsymbol{\\lambda}}\n\\def\\vmu{\\boldsymbol{\\mu}}\n\\def\\vnu{\\boldsymbol{\\nu}}\n\\def\\vxi{\\boldsymbol{\\xi}}\n\\def\\vpi{\\boldsymbol{\\pi}}\n\\def\\vrho{\\boldsymbol{\\rho}}\n\\def\\vsigma{\\boldsymbol{\\sigma}}\n\\def\\vtau{\\boldsymbol{\\tau}}\n\\def\\vupsilon{\\boldsymbol{\\upsilon}}\n\\def\\vphi{\\boldsymbol{\\phi}}\n\\def\\vchi{\\boldsymbol{\\chi}}\n\\def\\vpsi{\\boldsymbol{\\psi}}\n\\def\\vomega{\\boldsymbol{\\omega}}\n\\def\\vGamma{\\boldsymbol{\\Gamma}}\n\\def\\vDelta{\\boldsymbol{\\Delta}}\n\\def\\vTheta{\\boldsymbol{\\Theta}}\n\\def\\vLambda{\\boldsymbol{\\Lambda}}\n\\def\\vXi{\\boldsymbol{\\Xi}}\n\\def\\vPi{\\boldsymbol{\\Pi}}\n\\def\\vSigma{\\boldsymbol{\\Sigma}}\n\\def\\vUpsilon{\\boldsymbol{\\Upsilon}}\n\\def\\vPhi{\\boldsymbol{\\Phi}}\n\\def\\vPsi{\\boldsymbol{\\Psi}}\n\\def\\vOmega{\\boldsymbol{\\Omega}}\n\\def\\ra{\\mathbf{a}}\n\\def\\rb{\\mathbf{b}}\n\\def\\rc{\\mathbf{c}}\n\\def\\rd{\\mathbf{d}}\n\\def\\re{\\mathbf{e}}\n\\def\\rf{\\mathbf{f}}\n\\def\\rg{\\mathbf{g}}\n\\def\\rh{\\mathbf{h}}\n\\def\\ri{\\mathbf{i}}\n\\def\\rj{\\mathbf{j}}\n\\def\\rk{\\mathbf{k}}\n\\def\\rl{\\mathbf{l}}\n\\def\\rm{\\mathbf{m}}\n\\def\\rn{\\mathbf{n}}\n\\def\\ro{\\mathbf{o}}\n\\def\\rp{\\mathbf{p}}\n\\def\\rq{\\mathbf{q}}\n\\def\\rr{\\mathbf{r}}\n\\def\\rs{\\mathbf{s}}\n\\def\\rt{\\mathbf{t}}\n\\def\\ru{\\mathbf{u}}\n\\def\\rv{\\mathbf{v}}\n\\def\\rw{\\mathbf{w}}\n\\def\\rx{\\mathbf{x}}\n\\def\\ry{\\mathbf{y}}\n\\def\\rz{\\mathbf{z}}\n\\def\\vzero{\\mathbf{0}}\n\\def\\vone{\\mathbf{1}}\n\\def\\Ev{\\mathbb{E}}\n\\def\\Pr{\\operatorname{Pr}}\n\\def\\Var{\\operatorname{Var}}\n\\def\\Cov{\\operatorname{Cov}}\n\\def\\Cor{\\operatorname{Cor}}\n\\def\\Corr{\\operatorname{Corr}}\n\\def\\distBern{\\operatorname{Ber}}\n\\def\\distBin{\\operatorname{Bin}}\n\\def\\distGeom{\\operatorname{Geo}}\n\\def\\distNB{\\operatorname{NB}}\n\\def\\distPois{\\operatorname{Poi}}\n\\def\\distHyp{\\operatorname{Hyp}}\n\\def\\distUnif{\\operatorname{Unif}}\n\\def\\distExp{\\operatorname{Exp}}\n\\def\\distGamma{\\operatorname{Gamma}}\n\\def\\distN{\\mathcal{N}}\n\\def\\distBeta{\\operatorname{Beta}}\n\\def\\distCauchy{\\operatorname{Cauchy}}\n\\def\\distDir{\\operatorname{Dir}}\n\\def\\distMult{\\operatorname{Mult}}\n\\def\\distWishart{\\operatorname{Wishart}}\n\\def\\tr{\\operatorname{tr}}\n\\def\\trace{\\operatorname{tr}}\n\\def\\rank{\\operatorname{rank}}\n\\def\\nullity{\\operatorname{nullity}}\n\\def\\Span{\\operatorname{span}}\n\\def\\spn{\\operatorname{span}}\n\\def\\col{\\operatorname{col}}\n\\def\\row{\\operatorname{row}}\n\\def\\nul{\\operatorname{nul}}\n\\def\\img{\\operatorname{img}}\n\\def\\diag{\\operatorname{diag}}\n\\def\\dd{\\mathrm{d}}\n\\def\\transpose{^\\top}\n\\def\\inv{^{-1}}\n\\def\\abs#1{\\left\\lvert#1\\right\\rvert}\n\\def\\norm#1{\\left\\lVert#1\\right\\rVert}\n\\def\\floor#1{\\left\\lfloor#1\\right\\rfloor}\n\\def\\ceil#1{\\left\\lceil#1\\right\\rceil}\n\\def\\inner#1#2{\\langle #1, #2 \\rangle}\n\\def\\lcm{\\operatorname{lcm}}\n\\def\\im{\\operatorname{im}}\n\\def\\sgn{\\operatorname{sgn}}\n\\def\\argmax{\\operatorname{argmax}}\n\\def\\argmin{\\operatorname{argmin}}\n\\def\\softmax{\\operatorname{softmax}}\n\\def\\x{\\vx}\n\\def\\y{\\vy}\n\\def\\z{\\vz}\n\\def\\w{\\vw}\n\\def\\u{\\vu}\n\\def\\e{\\ve}\n\\def\\ba{\\mathbf{a}}\n\\def\\bb{\\mathbf{b}}\n\\def\\bc{\\mathbf{c}}\n\\def\\bd{\\mathbf{d}}\n\\def\\be{\\mathbf{e}}\n\\def\\bff{\\mathbf{f}}\n\\def\\bg{\\mathbf{g}}\n\\def\\bh{\\mathbf{h}}\n\\def\\bi{\\mathbf{i}}\n\\def\\bj{\\mathbf{j}}\n\\def\\bk{\\mathbf{k}}\n\\def\\bl{\\mathbf{l}}\n\\def\\bm{\\mathbf{m}}\n\\def\\bn{\\mathbf{n}}\n\\def\\bo{\\mathbf{o}}\n\\def\\bp{\\mathbf{p}}\n\\def\\bq{\\mathbf{q}}\n\\def\\br{\\mathbf{r}}\n\\def\\bs{\\mathbf{s}}\n\\def\\bt{\\mathbf{t}}\n\\def\\bu{\\mathbf{u}}\n\\def\\bv{\\mathbf{v}}\n\\def\\bw{\\mathbf{w}}\n\\def\\bx{\\mathbf{x}}\n\\def\\by{\\mathbf{y}}\n\\def\\bz{\\mathbf{z}}\n\\def\\bA{\\mathbf{A}}\n\\def\\bB{\\mathbf{B}}\n\\def\\bC{\\mathbf{C}}\n\\def\\bD{\\mathbf{D}}\n\\def\\bE{\\mathbf{E}}\n\\def\\bF{\\mathbf{F}}\n\\def\\bG{\\mathbf{G}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bJ{\\mathbf{J}}\n\\def\\bK{\\mathbf{K}}\n\\def\\bL{\\mathbf{L}}\n\\def\\bM{\\mathbf{M}}\n\\def\\bN{\\mathbf{N}}\n\\def\\bO{\\mathbf{O}}\n\\def\\bP{\\mathbf{P}}\n\\def\\bQ{\\mathbf{Q}}\n\\def\\bR{\\mathbf{R}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bT{\\mathbf{T}}\n\\def\\bU{\\mathbf{U}}\n\\def\\bV{\\mathbf{V}}\n\\def\\bW{\\mathbf{W}}\n\\def\\bX{\\mathbf{X}}\n\\def\\bY{\\mathbf{Y}}\n\\def\\bZ{\\mathbf{Z}}\n\\def\\bSigma{\\mathbf{\\Sigma}}\n\\def\\bLambda{\\mathbf{\\Lambda}}\n\\def\\bOmega{\\mathbf{\\Omega}}\n$$\n\nSome vector spaces might be “too big” and be unwieldy to deal with. Therefore, it is often convenient to look at “smaller vector spaces” inside a given vector space. These are called subspaces.\n\nDefinition 7.1 (Subspace) Let \\(V\\) be a vector space over \\(F\\). A subset \\(W\\) of \\(V\\) is called a subspace of \\(V\\), if \\(W\\) is also a vector space over \\(F\\) with addition and scalar multiplication inherited from \\(V\\).\n\n\nRemark. Let \\((V, +, \\cdot)\\) be a vector space over \\(F\\), and \\(W \\subseteq V\\). If \\((W, +, \\cdot)\\) is also a vector space over \\(F\\) then \\(W\\) is a subspace of \\(V\\).\n\n\nExample 7.1 (Trivial Subspaces) Given a vector space \\(V\\), then\n\n\\(V\\) is a subspace of \\(V\\) itself;\n\\(\\left\\{ \\vzero \\right\\}\\) is a subspace of \\(V\\) (also known as the “zero subspace”);\n\\(\\varnothing\\) is not a subspace of \\(V\\).\n\n\n\nTheorem 7.1 (Subspace Test) Let \\(V\\) be a vector space over \\(F\\) and \\(W \\subseteq V\\). Then \\(W\\) is a subspace of \\(V\\) if and only if the following are satisfied:\n\n\\(W\\) is non-empty;\n\\(W\\) is closed under addition: \\(\\vw_1 + \\vw_2 \\in W\\) for all \\(\\vw_1, \\vw_2 \\in W\\);\n\\(W\\) is closed under scalar multiplication: \\(\\alpha \\vw \\in W\\) for all \\(\\alpha \\in F\\) and \\(\\vw \\in W\\).\n\n\n\nProof. \\((\\Rightarrow)\\) If \\(W\\) is a subspace, then \\(W\\) is a vector space, so it contains the zero vector (hence non-empty) and is closed under addition and scalar multiplication by definition.\n\\((\\Leftarrow)\\) Assume the three conditions hold. Since \\(W \\subseteq V\\), the operations on \\(W\\) are inherited from \\(V\\), so associativity, commutativity, and distributivity are automatically satisfied. It remains to verify the existence of identity elements and inverses.\n\nZero vector: Since \\(W \\neq \\varnothing\\), there exists \\(\\vw \\in W\\). By closure under scalar multiplication, \\(0 \\cdot \\vw = \\vzero \\in W\\).\nAdditive inverse: For any \\(\\vw \\in W\\), closure under scalar multiplication gives \\((-1) \\cdot \\vw = -\\vw \\in W\\).\n\nThus \\(W\\) is a vector space under the inherited operations, i.e., a subspace of \\(V\\).\n\n\nRemark. Note that from the proof, the first condition of the Subspace Test can be replaced by the requirement that \\(W\\) contains the zero vector of \\(V\\).\n\n\nRemark. In practice, to test whether a given subset of a vector space is a subspace, we use Theorem 7.1 instead of checking all eight vector space axioms from scratch, which is much less tedious. Moreover, some introductory textbooks use Theorem 7.1 as the definition of a subspace. While this is logically equivalent, it can be somewhat less intuitive than defining a subspace as a subset that is itself a vector space.\n\n\nExample 7.2 (Line as Subspace) \\(W = \\left\\{ (x, y) \\in \\nR^2 : x + 2y = 0 \\right\\}\\) is a subspace of \\(\\nR^2\\). Indeed:\n\nZero vector: \\((0, 0) \\in W\\) since \\(0 + 2(0) = 0\\).\nAddition: If \\((x_1, y_1), (x_2, y_2) \\in W\\), then \\((x_1+x_2) + 2(y_1+y_2) = (x_1+2y_1) + (x_2+2y_2) = 0 + 0 = 0\\), so their sum is in \\(W\\).\nScalar multiplication: If \\((x, y) \\in W\\) and \\(c \\in \\nR\\), then \\((cx) + 2(cy) = c(x+2y) = c(0) = 0\\), so \\(c(x, y) \\in W\\).\n\n\n\nExample 7.3 (Translated Line is Not a Subspace) \\(W = \\left\\{ (x, y) \\in \\nR^2 : x + 2y = 3 \\right\\}\\) is not a subspace of \\(\\nR^2\\). This is because it does not contain the zero vector: \\(0 + 2(0) = 0 \\neq 3\\), so \\((0, 0) \\notin W\\).\n\n\nExample 7.4 (Polynomials of Bounded Degree) Let \\(F[x]\\) be the vector space of all polynomials over a field \\(F\\). For a fixed \\(n \\in \\nN\\), the set \\(F[x]_{\\leq n}\\) of polynomials of degree at most \\(n\\) is a subspace of \\(F[x]\\).\n\nZero vector: The zero polynomial has degree \\(-\\infty\\), so \\(0 \\in F[x]_{\\leq n}\\).\nAddition: If \\(\\deg p \\leq n\\) and \\(\\deg q \\leq n\\), then \\(\\deg(p+q) \\leq \\max(\\deg p, \\deg q) \\leq n\\).\nScalar multiplication: If \\(\\deg p \\leq n\\) and \\(c \\in F\\), then \\(\\deg(cp) \\leq \\deg p \\leq n\\).\n\n\n\nExample 7.5 (Polynomials of Exact Degree) The set \\(F[x]_{=n}\\) of polynomials of degree exactly \\(n\\) is not a subspace of \\(F[x]\\).\n\nIt does not contain the zero vector (since \\(\\deg 0 = -\\infty \\neq n\\)).\nIt is not closed under addition. For example, if \\(p(x) = x^n + x\\) and \\(q(x) = -x^n\\), both have degree \\(n\\), but \\(p(x) + q(x) = x\\), which has degree \\(1 \\neq n\\) (assuming \\(n &gt; 1\\)).\n\n\n\nExample 7.6 (Differentiable Functions) Let \\(C(\\nR) = \\{f : \\nR \\to \\nR \\mid f \\text{ is continuous}\\}\\) be the set of all continuous functions on \\(\\nR\\). We consider the set of differentiable functions: \\[\n    D(\\nR) = \\{ f : \\nR \\to \\nR \\mid f \\text{ is differentiable} \\}\n\\] To verify that \\(D(\\nR)\\) is a subspace of \\(C(\\nR)\\):\n\nSubset: Since differentiability implies continuity, we have \\(D(\\nR) \\subseteq C(\\nR)\\).\nZero vector: The zero function \\(f(x) = 0\\) is differentiable, so \\(\\vzero \\in D(\\nR)\\).\nClosure under addition: If \\(f, g \\in D(\\nR)\\), then \\(f+g\\) is differentiable with \\((f+g)' = f' + g'\\), so \\(f+g \\in D(\\nR)\\).\nClosure under scalar multiplication: If \\(f \\in D(\\nR)\\) and \\(c \\in \\nR\\), then \\(cf\\) is differentiable with \\((cf)' = c f'\\), so \\(cf \\in D(\\nR)\\).\n\nThus, \\(D(\\nR)\\) is a subspace of \\(C(\\nR)\\).\n\n\nExample 7.7 (Symmetric Matrices) The set of all symmetric matrices in \\(M_n(F)\\), denoted by \\(S_n(F) = \\{ \\mA \\in M_n(F) : \\mA = \\mA\\T \\}\\), is a subspace of \\(M_n(F)\\).\n\nZero vector: The zero matrix \\(\\mO\\) is symmetric since \\(\\mO\\T = \\mO\\), so \\(\\mO \\in S_n(F)\\).\nAddition: If \\(\\mA, \\mB \\in S_n(F)\\), then \\((\\mA+\\mB)\\T = \\mA\\T + \\mB\\T = \\mA + \\mB\\), so \\(\\mA+\\mB \\in S_n(F)\\).\nScalar multiplication: If \\(\\mA \\in S_n(F)\\) and \\(c \\in F\\), then \\((c\\mA)\\T = c\\mA\\T = c\\mA\\), so \\(c\\mA \\in S_n(F)\\).\n\n\n\nExample 7.8 (Trace-free Matrices) The set of all trace-free matrices in \\(M_n(F)\\), denoted by \\(W = \\{ \\mA \\in M_n(F) : \\tr(\\mA) = 0 \\}\\), is a subspace of \\(M_n(F)\\).\n\nZero vector: \\(\\tr(\\mO) = 0 + 0 + \\cdots + 0 = 0\\), so \\(\\mO \\in W\\).\nAddition: If \\(\\mA, \\mB \\in W\\), then \\(\\tr(\\mA+\\mB) = \\tr(\\mA) + \\tr(\\mB) = 0 + 0 = 0\\), so \\(\\mA+\\mB \\in W\\).\nScalar multiplication: If \\(\\mA \\in W\\) and \\(c \\in F\\), then \\(\\tr(c\\mA) = c \\tr(\\mA) = c(0) = 0\\), so \\(c\\mA \\in W\\).\n\n\n\nTheorem 7.2 (Intersection of Subspaces) Let \\(W_1\\) and \\(W_2\\) be subspaces of a vector space \\(V\\). Then the intersection \\(W_1 \\cap W_2\\) is also a subspace of \\(V\\).\n\n\nProof. We use the Subspace Test on \\(W = W_1 \\cap W_2\\):\n\nZero vector: Since \\(W_1\\) and \\(W_2\\) are subspaces, \\(\\vzero \\in W_1\\) and \\(\\vzero \\in W_2\\). Thus \\(\\vzero \\in W_1 \\cap W_2\\).\nClosure under addition: Let \\(\\vw, \\vz \\in W_1 \\cap W_2\\). Then \\(\\vw, \\vz \\in W_1\\) and \\(\\vw, \\vz \\in W_2\\). Since \\(W_1\\) and \\(W_2\\) are subspaces, they are closed under addition, so \\(\\vw + \\vz \\in W_1\\) and \\(\\vw + \\vz \\in W_2\\). Hence \\(\\vw + \\vz \\in W_1 \\cap W_2\\).\nClosure under scalar multiplication: Let \\(\\vw \\in W_1 \\cap W_2\\) and \\(c \\in F\\). Then \\(\\vw \\in W_1\\) and \\(\\vw \\in W_2\\). Since \\(W_1\\) and \\(W_2\\) are closed under scalar multiplication, \\(c\\vw \\in W_1\\) and \\(c\\vw \\in W_2\\). Hence \\(c\\vw \\in W_1 \\cap W_2\\).\n\nSince all three conditions of the Subspace Test are satisfied, \\(W_1 \\cap W_2\\) is a subspace of \\(V\\).\n\n\nRemark. Unlike the intersection, the union of two subspaces is not necessarily a subspace. For \\(W_1 \\cup W_2\\) to be a subspace, one subspace must be contained within the other (i.e., \\(W_1 \\subseteq W_2\\) or \\(W_2 \\subseteq W_1\\)).\n\n\nExample 7.9 (Union of Axes) Consider the vector space \\(V = \\nR^2\\). Let \\(W_1\\) be the \\(x\\)-axis and \\(W_2\\) be the \\(y\\)-axis: \\[\n    W_1 = \\{ (x, 0) : x \\in \\nR \\}, \\quad W_2 = \\{ (0, y) : y \\in \\nR \\}.\n\\] Both \\(W_1\\) and \\(W_2\\) are subspaces of \\(\\nR^2\\). However, their union \\(W_1 \\cup W_2\\) is not a subspace because it is not closed under addition.\nIndeed, \\((1, 0) \\in W_1 \\subseteq W_1 \\cup W_2\\) and \\((0, 1) \\in W_2 \\subseteq W_1 \\cup W_2\\), but their sum: \\[\n    (1, 0) + (0, 1) = (1, 1)\n\\] is not in \\(W_1 \\cup W_2\\) since \\((1, 1)\\) is neither on the \\(x\\)-axis nor the \\(y\\)-axis.\n\n\nProposition 7.1 (Union of Subspaces) Let \\(W_1, W_2\\) be subspaces of \\(V\\). Then \\(W_1 \\cup W_2\\) is a subspace of \\(V\\) if and only if \\(W_1 \\subseteq W_2\\) or \\(W_2 \\subseteq W_1\\).\n\n\nProof. \\((\\Leftarrow)\\) If \\(W_1 \\subseteq W_2\\), then \\(W_1 \\cup W_2 = W_2\\), which is a subspace. Similarly, if \\(W_2 \\subseteq W_1\\), the union is \\(W_1\\), which is also a subspace.\n\\((\\Rightarrow)\\) We prove the contrapositive: if neither is contained in the other, then the union is not a subspace. Assume \\(W_1 \\not\\subseteq W_2\\) and \\(W_2 \\not\\subseteq W_1\\).\n\nSince \\(W_1 \\not\\subseteq W_2\\), there exists a vector \\(\\vu \\in W_1\\) such that \\(\\vu \\notin W_2\\).\nSince \\(W_2 \\not\\subseteq W_1\\), there exists a vector \\(\\vv \\in W_2\\) such that \\(\\vv \\notin W_1\\).\n\nConsider the sum \\(\\vw = \\vu + \\vv\\). We show that \\(\\vw \\notin W_1 \\cup W_2\\):\n\nIf \\(\\vw \\in W_1\\), then \\(\\vv = \\vw - \\vu\\). Since \\(\\vw \\in W_1\\) and \\(\\vu \\in W_1\\), their difference \\(\\vv\\) must be in \\(W_1\\) (by closure). But we chose \\(\\vv \\notin W_1\\), which is a contradiction.\nIf \\(\\vw \\in W_2\\), then \\(\\vu = \\vw - \\vv\\). Since \\(\\vw \\in W_2\\) and \\(\\vv \\in W_2\\), their difference \\(\\vu\\) must be in \\(W_2\\). But we chose \\(\\vu \\notin W_2\\), which is a contradiction.\n\nTherefore, \\(\\vu + \\vv\\) is in neither \\(W_1\\) nor \\(W_2\\), so it is not in \\(W_1 \\cup W_2\\). Thus, the union is not closed under addition and is therefore not a subspace.",
    "crumbs": [
      "Vector Spaces and Dimensions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Subspaces</span>"
    ]
  },
  {
    "objectID": "src/ch01-vector-spaces/03-span.html",
    "href": "src/ch01-vector-spaces/03-span.html",
    "title": "8  Linear Combinations and Span",
    "section": "",
    "text": "$$\n\\def\\nR{\\mathbb{R}}\n\\def\\nC{\\mathbb{C}}\n\\def\\nN{\\mathbb{N}}\n\\def\\nZ{\\mathbb{Z}}\n\\def\\nQ{\\mathbb{Q}}\n\\def\\nF{\\mathbb{F}}\n\\def\\nP{\\mathbb{P}}\n\\def\\nE{\\mathbb{E}}\n\\def\\sA{\\mathcal{A}}\n\\def\\sB{\\mathcal{B}}\n\\def\\sC{\\mathcal{C}}\n\\def\\sD{\\mathcal{D}}\n\\def\\sE{\\mathcal{E}}\n\\def\\sF{\\mathcal{F}}\n\\def\\sG{\\mathcal{G}}\n\\def\\sH{\\mathcal{H}}\n\\def\\sL{\\mathcal{L}}\n\\def\\sM{\\mathcal{M}}\n\\def\\sN{\\mathcal{N}}\n\\def\\sO{\\mathcal{O}}\n\\def\\sP{\\mathcal{P}}\n\\def\\sS{\\mathcal{S}}\n\\def\\sT{\\mathcal{T}}\n\\def\\sX{\\mathcal{X}}\n\\def\\sY{\\mathcal{Y}}\n\\def\\va{\\mathbf{a}}\n\\def\\vb{\\mathbf{b}}\n\\def\\vc{\\mathbf{c}}\n\\def\\vd{\\mathbf{d}}\n\\def\\ve{\\mathbf{e}}\n\\def\\vf{\\mathbf{f}}\n\\def\\vg{\\mathbf{g}}\n\\def\\vh{\\mathbf{h}}\n\\def\\vi{\\mathbf{i}}\n\\def\\vj{\\mathbf{j}}\n\\def\\vk{\\mathbf{k}}\n\\def\\vl{\\mathbf{l}}\n\\def\\vm{\\mathbf{m}}\n\\def\\vn{\\mathbf{n}}\n\\def\\vo{\\mathbf{o}}\n\\def\\vp{\\mathbf{p}}\n\\def\\vq{\\mathbf{q}}\n\\def\\vr{\\mathbf{r}}\n\\def\\vs{\\mathbf{s}}\n\\def\\vt{\\mathbf{t}}\n\\def\\vu{\\mathbf{u}}\n\\def\\vv{\\mathbf{v}}\n\\def\\vw{\\mathbf{w}}\n\\def\\vx{\\mathbf{x}}\n\\def\\vy{\\mathbf{y}}\n\\def\\vz{\\mathbf{z}}\n\\def\\mA{\\mathbf{A}}\n\\def\\mB{\\mathbf{B}}\n\\def\\mC{\\mathbf{C}}\n\\def\\mD{\\mathbf{D}}\n\\def\\mE{\\mathbf{E}}\n\\def\\mF{\\mathbf{F}}\n\\def\\mG{\\mathbf{G}}\n\\def\\mH{\\mathbf{H}}\n\\def\\mI{\\mathbf{I}}\n\\def\\mJ{\\mathbf{J}}\n\\def\\mK{\\mathbf{K}}\n\\def\\mL{\\mathbf{L}}\n\\def\\mM{\\mathbf{M}}\n\\def\\mN{\\mathbf{N}}\n\\def\\mO{\\mathbf{O}}\n\\def\\mP{\\mathbf{P}}\n\\def\\mQ{\\mathbf{Q}}\n\\def\\mR{\\mathbf{R}}\n\\def\\mS{\\mathbf{S}}\n\\def\\mT{\\mathbf{T}}\n\\def\\mU{\\mathbf{U}}\n\\def\\mV{\\mathbf{V}}\n\\def\\mW{\\mathbf{W}}\n\\def\\mX{\\mathbf{X}}\n\\def\\mY{\\mathbf{Y}}\n\\def\\mZ{\\mathbf{Z}}\n\\def\\A{\\mathbf{A}}\n\\def\\B{\\mathbf{B}}\n\\def\\C{\\mathbf{C}}\n\\def\\D{\\mathbf{D}}\n\\def\\E{\\mathbf{E}}\n\\def\\F{\\mathbf{F}}\n\\def\\G{\\mathbf{G}}\n\\def\\H{\\mathbf{H}}\n\\def\\I{\\mathbf{I}}\n\\def\\J{\\mathbf{J}}\n\\def\\K{\\mathbf{K}}\n\\def\\L{\\mathbf{L}}\n\\def\\M{\\mathbf{M}}\n\\def\\N{\\mathbf{N}}\n\\def\\O{\\mathbf{O}}\n\\def\\P{\\mathbf{P}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\R{\\mathbf{R}}\n\\def\\S{\\mathbf{S}}\n\\def\\T{^\\top}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\W{\\mathbf{W}}\n\\def\\X{\\mathbf{X}}\n\\def\\Y{\\mathbf{Y}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\valpha{\\boldsymbol{\\alpha}}\n\\def\\vbeta{\\boldsymbol{\\beta}}\n\\def\\vgamma{\\boldsymbol{\\gamma}}\n\\def\\vdelta{\\boldsymbol{\\delta}}\n\\def\\vepsilon{\\boldsymbol{\\epsilon}}\n\\def\\vzeta{\\boldsymbol{\\zeta}}\n\\def\\veta{\\boldsymbol{\\eta}}\n\\def\\vtheta{\\boldsymbol{\\theta}}\n\\def\\viota{\\boldsymbol{\\iota}}\n\\def\\vkappa{\\boldsymbol{\\kappa}}\n\\def\\vlambda{\\boldsymbol{\\lambda}}\n\\def\\vmu{\\boldsymbol{\\mu}}\n\\def\\vnu{\\boldsymbol{\\nu}}\n\\def\\vxi{\\boldsymbol{\\xi}}\n\\def\\vpi{\\boldsymbol{\\pi}}\n\\def\\vrho{\\boldsymbol{\\rho}}\n\\def\\vsigma{\\boldsymbol{\\sigma}}\n\\def\\vtau{\\boldsymbol{\\tau}}\n\\def\\vupsilon{\\boldsymbol{\\upsilon}}\n\\def\\vphi{\\boldsymbol{\\phi}}\n\\def\\vchi{\\boldsymbol{\\chi}}\n\\def\\vpsi{\\boldsymbol{\\psi}}\n\\def\\vomega{\\boldsymbol{\\omega}}\n\\def\\vGamma{\\boldsymbol{\\Gamma}}\n\\def\\vDelta{\\boldsymbol{\\Delta}}\n\\def\\vTheta{\\boldsymbol{\\Theta}}\n\\def\\vLambda{\\boldsymbol{\\Lambda}}\n\\def\\vXi{\\boldsymbol{\\Xi}}\n\\def\\vPi{\\boldsymbol{\\Pi}}\n\\def\\vSigma{\\boldsymbol{\\Sigma}}\n\\def\\vUpsilon{\\boldsymbol{\\Upsilon}}\n\\def\\vPhi{\\boldsymbol{\\Phi}}\n\\def\\vPsi{\\boldsymbol{\\Psi}}\n\\def\\vOmega{\\boldsymbol{\\Omega}}\n\\def\\ra{\\mathbf{a}}\n\\def\\rb{\\mathbf{b}}\n\\def\\rc{\\mathbf{c}}\n\\def\\rd{\\mathbf{d}}\n\\def\\re{\\mathbf{e}}\n\\def\\rf{\\mathbf{f}}\n\\def\\rg{\\mathbf{g}}\n\\def\\rh{\\mathbf{h}}\n\\def\\ri{\\mathbf{i}}\n\\def\\rj{\\mathbf{j}}\n\\def\\rk{\\mathbf{k}}\n\\def\\rl{\\mathbf{l}}\n\\def\\rm{\\mathbf{m}}\n\\def\\rn{\\mathbf{n}}\n\\def\\ro{\\mathbf{o}}\n\\def\\rp{\\mathbf{p}}\n\\def\\rq{\\mathbf{q}}\n\\def\\rr{\\mathbf{r}}\n\\def\\rs{\\mathbf{s}}\n\\def\\rt{\\mathbf{t}}\n\\def\\ru{\\mathbf{u}}\n\\def\\rv{\\mathbf{v}}\n\\def\\rw{\\mathbf{w}}\n\\def\\rx{\\mathbf{x}}\n\\def\\ry{\\mathbf{y}}\n\\def\\rz{\\mathbf{z}}\n\\def\\vzero{\\mathbf{0}}\n\\def\\vone{\\mathbf{1}}\n\\def\\Ev{\\mathbb{E}}\n\\def\\Pr{\\operatorname{Pr}}\n\\def\\Var{\\operatorname{Var}}\n\\def\\Cov{\\operatorname{Cov}}\n\\def\\Cor{\\operatorname{Cor}}\n\\def\\Corr{\\operatorname{Corr}}\n\\def\\distBern{\\operatorname{Ber}}\n\\def\\distBin{\\operatorname{Bin}}\n\\def\\distGeom{\\operatorname{Geo}}\n\\def\\distNB{\\operatorname{NB}}\n\\def\\distPois{\\operatorname{Poi}}\n\\def\\distHyp{\\operatorname{Hyp}}\n\\def\\distUnif{\\operatorname{Unif}}\n\\def\\distExp{\\operatorname{Exp}}\n\\def\\distGamma{\\operatorname{Gamma}}\n\\def\\distN{\\mathcal{N}}\n\\def\\distBeta{\\operatorname{Beta}}\n\\def\\distCauchy{\\operatorname{Cauchy}}\n\\def\\distDir{\\operatorname{Dir}}\n\\def\\distMult{\\operatorname{Mult}}\n\\def\\distWishart{\\operatorname{Wishart}}\n\\def\\tr{\\operatorname{tr}}\n\\def\\trace{\\operatorname{tr}}\n\\def\\rank{\\operatorname{rank}}\n\\def\\nullity{\\operatorname{nullity}}\n\\def\\Span{\\operatorname{span}}\n\\def\\spn{\\operatorname{span}}\n\\def\\col{\\operatorname{col}}\n\\def\\row{\\operatorname{row}}\n\\def\\nul{\\operatorname{nul}}\n\\def\\img{\\operatorname{img}}\n\\def\\diag{\\operatorname{diag}}\n\\def\\dd{\\mathrm{d}}\n\\def\\transpose{^\\top}\n\\def\\inv{^{-1}}\n\\def\\abs#1{\\left\\lvert#1\\right\\rvert}\n\\def\\norm#1{\\left\\lVert#1\\right\\rVert}\n\\def\\floor#1{\\left\\lfloor#1\\right\\rfloor}\n\\def\\ceil#1{\\left\\lceil#1\\right\\rceil}\n\\def\\inner#1#2{\\langle #1, #2 \\rangle}\n\\def\\lcm{\\operatorname{lcm}}\n\\def\\im{\\operatorname{im}}\n\\def\\sgn{\\operatorname{sgn}}\n\\def\\argmax{\\operatorname{argmax}}\n\\def\\argmin{\\operatorname{argmin}}\n\\def\\softmax{\\operatorname{softmax}}\n\\def\\x{\\vx}\n\\def\\y{\\vy}\n\\def\\z{\\vz}\n\\def\\w{\\vw}\n\\def\\u{\\vu}\n\\def\\e{\\ve}\n\\def\\ba{\\mathbf{a}}\n\\def\\bb{\\mathbf{b}}\n\\def\\bc{\\mathbf{c}}\n\\def\\bd{\\mathbf{d}}\n\\def\\be{\\mathbf{e}}\n\\def\\bff{\\mathbf{f}}\n\\def\\bg{\\mathbf{g}}\n\\def\\bh{\\mathbf{h}}\n\\def\\bi{\\mathbf{i}}\n\\def\\bj{\\mathbf{j}}\n\\def\\bk{\\mathbf{k}}\n\\def\\bl{\\mathbf{l}}\n\\def\\bm{\\mathbf{m}}\n\\def\\bn{\\mathbf{n}}\n\\def\\bo{\\mathbf{o}}\n\\def\\bp{\\mathbf{p}}\n\\def\\bq{\\mathbf{q}}\n\\def\\br{\\mathbf{r}}\n\\def\\bs{\\mathbf{s}}\n\\def\\bt{\\mathbf{t}}\n\\def\\bu{\\mathbf{u}}\n\\def\\bv{\\mathbf{v}}\n\\def\\bw{\\mathbf{w}}\n\\def\\bx{\\mathbf{x}}\n\\def\\by{\\mathbf{y}}\n\\def\\bz{\\mathbf{z}}\n\\def\\bA{\\mathbf{A}}\n\\def\\bB{\\mathbf{B}}\n\\def\\bC{\\mathbf{C}}\n\\def\\bD{\\mathbf{D}}\n\\def\\bE{\\mathbf{E}}\n\\def\\bF{\\mathbf{F}}\n\\def\\bG{\\mathbf{G}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bJ{\\mathbf{J}}\n\\def\\bK{\\mathbf{K}}\n\\def\\bL{\\mathbf{L}}\n\\def\\bM{\\mathbf{M}}\n\\def\\bN{\\mathbf{N}}\n\\def\\bO{\\mathbf{O}}\n\\def\\bP{\\mathbf{P}}\n\\def\\bQ{\\mathbf{Q}}\n\\def\\bR{\\mathbf{R}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bT{\\mathbf{T}}\n\\def\\bU{\\mathbf{U}}\n\\def\\bV{\\mathbf{V}}\n\\def\\bW{\\mathbf{W}}\n\\def\\bX{\\mathbf{X}}\n\\def\\bY{\\mathbf{Y}}\n\\def\\bZ{\\mathbf{Z}}\n\\def\\bSigma{\\mathbf{\\Sigma}}\n\\def\\bLambda{\\mathbf{\\Lambda}}\n\\def\\bOmega{\\mathbf{\\Omega}}\n$$\n\nIn the previous section, we studied subspaces. A natural question arises: given a set of vectors, how can we describe the “smallest” subspace that contains them? This leads us to the concept of linear combinations and the span.\n\nDefinition 8.1 (Linear Combination) Let \\(V\\) be a vector space over \\(F\\), \\(S = \\{ \\vv_1, \\vv_2, \\dots, \\vv_m \\}\\) be a subset of \\(V\\). Then the expression \\[\n    a_1 \\vv_1 + a_2 \\vv_2 + \\cdots + a_m \\vv_m\n\\] is called a linear combination of \\(S\\). The scalars \\(a_i\\) are called the coefficients of the linear combination.\n\n\nExample 8.1 (Examples of Linear Combinations)  \n\nThe vector \\((2, 1, 0)\\) in \\(\\nR^3\\) is a linear combination of \\(S = \\{(1, 2, 3), (4, 5, 6)\\}\\) because \\[\n(2, 1, 0) = -2 \\cdot (1, 2, 3) + 1 \\cdot (4, 5, 6).\n\\]\nThe polynomial \\(f(x) = x^3 + 2x + 1\\) in \\(\\mathbb{R}[x]\\) is a linear combination of \\(S = \\{1, x, x^2, x^3\\}\\) because \\[\nf(x) = 1 \\cdot 1 + 2 \\cdot x + 0 \\cdot x^2 + 1 \\cdot x^3.\n\\]\n\n\n\nDefinition 8.2 (Span) Let \\(S\\) be a subset of a vector space \\(V\\) over \\(F\\). The span of \\(S\\), denoted by \\(\\Span_F(S)\\), is the set of all linear combinations of finite subsets of elements of \\(S\\).\nIf \\(S = \\{ \\vv_1, \\vv_2, \\dots, \\vv_n \\}\\), then \\(\\Span_F(S) = \\{ a_1 \\vv_1 + a_2 \\vv_2 + \\dots + a_n \\vv_n : a_i \\in F \\}\\). By convention, \\(\\Span_F(\\varnothing) = \\{ \\vzero \\}\\).\n\n\nRemark. When the underlying field is clear, we abuse the notation of \\(\\Span_F\\) and write it as \\(\\Span\\).\n\n\nTheorem 8.1 (Spanning Set as Subspace) The span of any subset \\(S \\subseteq V\\) is a subspace of \\(V\\). Moreover, it is the smallest subspace of \\(V\\) containing \\(S\\).\n\n\nProof. Let \\(W = \\Span(S)\\).\n\nZero vector: Since \\(\\Span(\\varnothing) = \\{ \\vzero \\}\\), and for non-empty \\(S\\), taking all coefficients as zero gives \\(\\vzero\\), we have \\(\\vzero \\in W\\).\nAddition: Let \\(\\vx, \\vy \\in W\\). Then \\(\\vx = \\sum a_i \\vv_i\\) and \\(\\vy = \\sum b_i \\vv_i\\). Their sum \\(\\vx + \\vy = \\sum (a_i + b_i) \\vv_i\\) is also a linear combination of elements in \\(S\\), so \\(\\vx + \\vy \\in W\\).\nScalar multiplication: For any \\(c \\in F\\), \\(c\\vx = \\sum (ca_i) \\vv_i \\in W\\).\n\nTo see it is the smallest subspace, note that any subspace containing \\(S\\) must be closed under addition and scalar multiplication, and thus must contain all linear combinations of elements in \\(S\\).\n\n\nDefinition 8.3 (Spanning Set) If \\(\\Span(S) = V\\), we say that \\(S\\) spans \\(V\\), or that \\(S\\) is a spanning set for \\(V\\).\n\n\nExample 8.2 (Spanning Set for Polynomials) The set \\(\\{ 1, x, x^2 \\}\\) spans \\(\\nR[x]_{\\leq 2}\\), the space of polynomials of degree at most 2.",
    "crumbs": [
      "Vector Spaces and Dimensions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Linear Combinations and Span</span>"
    ]
  },
  {
    "objectID": "src/ch01-vector-spaces/04-independence.html",
    "href": "src/ch01-vector-spaces/04-independence.html",
    "title": "9  Linear Independence",
    "section": "",
    "text": "9.1 The Redundant Members\nA spanning set is like a toolbox that is guaranteed to have every tool you need. But a messy toolbox might have three different hammers that all do the same thing. In mathematics, as in engineering, we value efficiency.\nIf one of the vectors in your spanning set can already be built using the others, then that vector is “dead weight”—it isn’t helping you reach any new territory. We call such a set linearly dependent.\nIn this section, we search for the “cleanest” possible sets. We want to know: “Is every vector in this collection actually contributing something unique, or is someone just coasting on the work of the others?”\nHaving the last remark in mind, we can develop an intuition that says linear independence of a set is equivalent to requiring the set containing no “redundant members.” Next we want to develop the intuition saying that throwing away these “redundant members” will not change the linear span.",
    "crumbs": [
      "Vector Spaces and Dimensions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear Independence</span>"
    ]
  },
  {
    "objectID": "src/ch01-vector-spaces/04-independence.html#the-redundant-members",
    "href": "src/ch01-vector-spaces/04-independence.html#the-redundant-members",
    "title": "9  Linear Independence",
    "section": "",
    "text": "Theorem 9.2 (Span Preservation) Let \\(S = \\{ \\vv_1, \\dots, \\vv_m, \\vw \\} \\subseteq V\\). If \\(\\vw \\in \\Span \\{ \\vv_1, \\dots, \\vv_m \\}\\), then \\[\n    \\Span(S) = \\Span \\{ \\vv_1, \\dots, \\vv_m \\}.\n\\]\n\n\nProof. Let \\(W = \\Span \\{ \\vv_1, \\dots, \\vv_m \\}\\) and \\(W' = \\Span(S)\\). Since \\(\\{ \\vv_1, \\dots, \\vv_m \\} \\subseteq S\\), it is clear that \\(W \\subseteq W'\\).\nFor the reverse inclusion, let \\(\\vv \\in W'\\). Then \\(\\vv\\) is a linear combination of vectors in \\(S\\): \\[\n    \\vv = a_1 \\vv_1 + \\dots + a_m \\vv_m + b \\vw\n\\] for some scalars \\(a_i, b\\). Since \\(\\vw \\in W\\), there exist scalars \\(c_1, \\dots, c_m\\) such that \\(\\vw = c_1 \\vv_1 + \\dots + c_m \\vv_m\\). Substituting this into the expression for \\(\\vv\\): \\[\n    \\vv = a_1 \\vv_1 + \\dots + a_m \\vv_m + b(c_1 \\vv_1 + \\dots + c_m \\vv_m) = (a_1 + bc_1)\\vv_1 + \\dots + (a_m + bc_m)\\vv_m.\n\\] Thus \\(\\vv\\) is a linear combination of \\(\\vv_1, \\dots, \\vv_m\\), so \\(\\vv \\in W\\). This shows \\(W' \\subseteq W\\). Hence \\(W' = W\\).",
    "crumbs": [
      "Vector Spaces and Dimensions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear Independence</span>"
    ]
  },
  {
    "objectID": "src/ch01-vector-spaces/05-basis.html",
    "href": "src/ch01-vector-spaces/05-basis.html",
    "title": "10  Bases",
    "section": "",
    "text": "10.1 Unique Representation and Coordinates\nWe have reached what many consider the “Goldilocks Zone” of Linear Algebra.\nIf a set is too small, it won’t span the whole space—you’ll be left with points you can’t reach. If a set is too large, it will be linearly dependent—you’ll have redundant vectors cluttering your workspace. A basis is a set that is “just right.” It is large enough to build everything, but small enough that every piece is essential.\nBecause a basis has no redundancy, it gives us something incredible: a unique address system. In an abstract vector space, it’s hard to tell someone where a vector is. But once we pick a basis, every vector can be described by a unique list of numbers—its coordinates. A basis is the bridge that allows us to turn abstract geometry into concrete arithmetic.\nThe most important property of a basis is that every vector in the space has a unique “address” relative to it.",
    "crumbs": [
      "Vector Spaces and Dimensions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bases</span>"
    ]
  },
  {
    "objectID": "src/ch01-vector-spaces/05-basis.html#unique-representation-and-coordinates",
    "href": "src/ch01-vector-spaces/05-basis.html#unique-representation-and-coordinates",
    "title": "10  Bases",
    "section": "",
    "text": "Theorem 10.1 (Unique Representation) Let \\(B = \\{ \\vv_1, \\dots, \\vv_n \\}\\) be a basis for \\(V\\). Then every vector \\(\\vv \\in V\\) can be written as a linear combination of elements in \\(B\\) in exactly one way.\n\n\nProof. Since \\(B\\) spans \\(V\\), there exist scalars \\(a_i\\) such that \\(\\vv = \\sum a_i \\vv_i\\). Suppose there is another representation \\(\\vv = \\sum b_i \\vv_i\\). Then: \\[\n    \\vzero = \\vv - \\vv = \\sum a_i \\vv_i - \\sum b_i \\vv_i = \\sum (a_i - b_i) \\vv_i.\n\\] Since \\(B\\) is linearly independent, we must have \\(a_i - b_i = 0\\) for all \\(i\\), meaning \\(a_i = b_i\\).\n\n\nDefinition 10.2 (Coordinates) Let \\(B = \\{ \\vv_1, \\dots, \\vv_n \\}\\) be an ordered basis for \\(V\\). For any \\(\\vv \\in V\\), the unique scalars \\(c_1, \\dots, c_n\\) such that \\(\\vv = \\sum c_i \\vv_i\\) are called the coordinates of \\(\\vv\\) with respect to \\(B\\). We write this as a column vector: \\[\n    [\\vv]_B = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix}.\n\\]\n\n\nRemark. The map \\(\\vv \\mapsto [\\vv]_B\\) provides a way to treat any abstract \\(n\\)-dimensional vector space as if it were simply \\(F^n\\). This is the power of a basis.",
    "crumbs": [
      "Vector Spaces and Dimensions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bases</span>"
    ]
  },
  {
    "objectID": "src/ch01-vector-spaces/06-dimension.html",
    "href": "src/ch01-vector-spaces/06-dimension.html",
    "title": "11  Sifting and the Replacement Theorem",
    "section": "",
    "text": "11.1 Consequences of the Replacement Theorem\nHow “big” is a vector space? We intuitively know that a plane is “bigger” than a line, and 3D space is “bigger” than a plane. But in the abstract world of vector spaces, we need a rigorous way to measure this.\nThe answer lies in the number of vectors in a basis. But there’s a potential problem: what if one person finds a basis with 3 vectors, and another person finds a basis for the same space with 4 vectors? If that were possible, our entire concept of “size” would collapse.\nIn this section, we prove the most important technical result of the chapter: the Steinitz Replacement Theorem. This theorem is the “engine” that guarantees every basis of a space has the exact same number of vectors. Once we have this, we can finally define the Dimension of a space—a single number that captures its fundamental complexity.\nHaving proved the Steinitz Replacement Theorem, the major conclusion we can draw from it is that any (finite) basis of the same vector space will have the same size.\nAn immediate consequence of the above theorem is the following:",
    "crumbs": [
      "Vector Spaces and Dimensions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Sifting and the Replacement Theorem</span>"
    ]
  },
  {
    "objectID": "src/ch01-vector-spaces/06-dimension.html#consequences-of-the-replacement-theorem",
    "href": "src/ch01-vector-spaces/06-dimension.html#consequences-of-the-replacement-theorem",
    "title": "11  Sifting and the Replacement Theorem",
    "section": "",
    "text": "Theorem 11.3 (Basis Theorem) If a vector space \\(V\\) over \\(F\\) has a finite basis, then every basis of \\(V\\) has the same number of vectors.\n\n\nProof. Let \\(B_1\\) and \\(B_2\\) be two bases with \\(n\\) and \\(m\\) vectors respectively.\n\nSince \\(B_1\\) is linearly independent and \\(B_2\\) spans, by Replacement Theorem, \\(n \\leq m\\).\nSince \\(B_2\\) is linearly independent and \\(B_1\\) spans, by Replacement Theorem, \\(m \\leq n\\).\n\nThus \\(n = m\\).\n\n\nDefinition 11.1 (Dimension) A vector space \\(V\\) over \\(F\\) is called finite-dimensional if it has a finite basis. The dimension of \\(V\\), denoted \\(\\dim_F (V)\\), is the number of vectors in any basis for \\(V\\). By convention, \\(\\dim_F (\\{\\vzero\\}) = 0\\).\n\n\nRemark. When the underlying field is clear, we abuse the notation of \\(\\dim_F\\) and write it as \\(\\dim\\).\n\n\nExample 11.2 (Examples of Dimensions) \\(\\dim(F^n) = n\\), \\(\\dim(F[x]_{\\leq n}) = n+1\\), and \\(\\dim(M_{m \\times n}(F)) = mn\\).\n\n\nTheorem 11.4 (Size Bounds for Independent and Spanning Sets) Let \\(V\\) be a vector space of dimension \\(n\\) over \\(F\\), and \\(S \\subseteq V\\) be a subset.\n\nIf \\(|S| &gt; n\\), then \\(S\\) cannot be linearly independent.\nIf \\(|S| &lt; n\\), then \\(S\\) cannot span \\(V\\).\n\n\n\nProof. \n\nGiven \\(|S| &gt; n\\), and suppose for the sake of contradiction that \\(S\\) is linearly independent. Let \\(B\\) be a basis of \\(V\\), then we have \\(|B| = n\\) and \\(B\\) is a spanning set of \\(V\\). By Replacement Theorem, we have \\[\n\\underbrace{|S|}_{\\text{size of lin. indep. set}} \\leq \\underbrace{|B|}_{\\text{size of spanning set}} = n.\n\\] Giving a contradiction, hence \\(S\\) cannot be linearly independent.\nGiven \\(|S| &lt; n\\), and suppose for the sake of contradiction that \\(S\\) spans \\(V\\). Let \\(B\\) be a basis of \\(V\\), then we have \\(|B| = n\\) and \\(B\\) is linearly independent. By Replacement Theorem, we have \\[\nn = \\underbrace{|B|}_{\\text{size of lin. indep. set}} \\leq \\underbrace{|S|}_{\\text{size of spanning set}}.\n\\] This implies \\(n \\leq |S|\\), which contradicts the assumption that \\(|S| &lt; n\\). Hence \\(S\\) cannot span \\(V\\).\n\n\n\nRemark. \\[\n    \\text{Size of lin. indep. set } \\leq \\text{ Size of basis } \\leq \\text{ Size of spanning set}.\n\\]\n\n\nTheorem 11.5 (Basis Extension Theorem) Let \\(V\\) be a finite-dimensional vector space and \\(S\\) be a linearly independent subset of \\(V\\). Then \\(S\\) can be extended into a basis of \\(V\\).\n\n\nProof. Since \\(V\\) is finite-dimensional, it has a finite basis \\(B = \\{ \\vv_1, \\dots, \\vv_n \\}\\). Consider the set \\(S \\cup B\\). Since \\(B\\) spans \\(V\\), \\(S \\cup B\\) also spans \\(V\\). We apply the Sifting Method (Theorem 11.1) to the ordered set formed by listing the elements of \\(S\\) first, followed by the elements of \\(B\\).\nThe Sifting Method will keep all elements of \\(S\\) because \\(S\\) is linearly independent. It then proceeds to examine each element of \\(B\\), keeping those that are not in the span of the vectors already kept. The resulting subset \\(S'\\) spans \\(\\Span(S \\cup B) = V\\) and is linearly independent by construction. Thus, \\(S'\\) is a basis for \\(V\\) that contains \\(S\\).\n\n\nCorollary 11.1 (Existence of Basis) Every finite spanning set of a vector space \\(V\\) contains a basis.\n\n\nProof. Apply the Sifting Method to the spanning set. The resulting subset \\(S'\\) is linearly independent by construction and still spans \\(V\\). Therefore, \\(S'\\) is a basis for \\(V\\).\n\n\nTheorem 11.6 (Dimension Implies Equality) Let \\(V\\) and \\(W\\) be finite-dimensional vector spaces over \\(F\\). If\n\n\\(W \\subseteq V\\);\n\\(\\dim_F (W) = \\dim_F (V)\\),\n\nthen \\(W = V\\).\n\n\nProof. If we show \\(V \\subseteq W\\) then we are done. Suppose for contradiction that \\(V \\nsubseteq W\\). That means there exists some \\(\\vv \\in V\\) that \\(\\vv \\notin W\\).\nLet \\(S = \\{ \\vw_1, \\dots, \\vw_n \\}\\) be a basis of \\(W\\), (consequently, \\(\\dim_F (W) = n\\)). Since \\(\\vv \\notin W\\), we have \\(\\vv \\notin \\Span(S)\\).\nThat would mean that \\(S' = \\{ \\vw_1, \\dots, \\vw_n, \\vv \\}\\) is linearly independent in \\(V\\). By Replacement Theorem, we have \\[\n    \\underbrace{|S'|}_{\\text{size of lin. indep. set}} \\leq \\dim_F (V),\n\\] which implies \\(n + 1 \\leq n\\), which is a contradiction, therefore \\(V \\subseteq W\\).\n\n\nRemark. This theorem makes proving \\(W = V\\) somewhat easier. As if we wanted to prove equivalence before, we can only do it by proving \\(W \\subseteq V\\) and \\(V \\subseteq W\\). Now we can just prove one side and then state that their dimension is the same.\n\n\n\nTheorem 11.7 (Maximal Independent Subset is a Basis) Let \\(V\\) be a vector space of dimension \\(n\\) over \\(F\\). Then any \\(n\\) independent vectors form a basis of \\(V\\). (We say that “any maximal linearly independent subset forms a basis”).\n\n\nProof. Let \\(S\\) be a linearly independent set of size \\(n\\). We want to show that \\(\\Span(S) = V\\).\nLet \\(W = \\Span(S)\\), this implies that \\(W\\) is a subspace of \\(V\\), which implies \\(W \\subseteq V\\).\nNotice that \\(S\\) spans \\(W\\) and \\(S\\) is linearly independent, therefore \\(S\\) is a basis of \\(W\\). So \\(\\dim_F (W) = |S| = n = \\dim_F (V)\\). By Theorem 11.6, we have \\(W = V\\), then \\(\\Span(S) = V\\).",
    "crumbs": [
      "Vector Spaces and Dimensions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Sifting and the Replacement Theorem</span>"
    ]
  }
]